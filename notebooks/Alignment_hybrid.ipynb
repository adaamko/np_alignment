{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from baseline_utils import process_baseline\n",
    "from nltk.corpus import stopwords\n",
    "import pprint\n",
    "from Levenshtein import distance\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "import io\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/adaamko/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = defaultdict(list)\n",
    "with open(\"/home/adaamko/tools/wikt2dict/dat/wiktionary/Hungarian/dictionary\", \"r+\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip().split(\"\\t\")\n",
    "        if line[0] == \"en\":\n",
    "            dictionary[line[1].lower()].append(line[3].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "dictionary_hok = defaultdict(list)\n",
    "count=0\n",
    "with open(\"/home/adaamko/data/hokoto\", errors=\"replace\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip().split(\"@\")\n",
    "        count+=1\n",
    "        dictionary_hok[line[0]].append(line[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "old_char = [\"a1\", \"e1\", \"u1\", \"i1\", \"o1\", \"A1\", \"E1\", \"U1\", \"I1\", \"O1\", \"o2\", \"u2\", \"O2\", \"U2\", \"o3\", \"u3\", \"O3\", \"U3\", \"_\"]\n",
    "new_char = [\"á\", \"é\", \"ú\", \"í\", \"ó\", \"Á\", \"É\", \"Ú\", \"Í\", \"Ó\", \"ö\", \"ü\", \"Ö\", \"Ü\", \"ő\", \"ű\", \"Ő\", \"Ű\", \" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in dictionary_hok:\n",
    "    for j in range(len(dictionary_hok[i])):\n",
    "        for k in range(len(old_char)):\n",
    "            dictionary_hok[i][j] = dictionary_hok[i][j].replace(old_char[k], new_char[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for word_hok in dictionary_hok:\n",
    "    words = dictionary_hok[word_hok]\n",
    "    dictionary[word_hok] += words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6567"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = process_baseline(\"/home/adaamko/data/1984.sen-aligned.np-aligned.gold\")\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop_sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-61f5d8749f13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'stop_sentences' is not defined"
     ]
    }
   ],
   "source": [
    "len(stop_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import hu_core_ud_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nlp_hu = hu_core_ud_lg.load()\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import emmorphpy.emmorphpy as emmorph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lem_hu = emmorph.EmMorphPy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_sentences = []\n",
    "\n",
    "for i,sentence in enumerate(sentences):\n",
    "    stop_count_en = 0\n",
    "    stop_count_hu = 0\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    \n",
    "    for s in sentence[\"en_sen\"]:\n",
    "        if type(s) == tuple:\n",
    "            doc = nlp_en(' '.join(s[1]))\n",
    "            nps = []\n",
    "            for ent in doc:\n",
    "                if not ent.is_stop:\n",
    "                    nps.append(ent.lemma_)\n",
    "            if not nps:\n",
    "                stop_count_en += 1\n",
    "                \n",
    "    for s in sentence[\"hu_sen\"]:\n",
    "        if type(s) == tuple:\n",
    "            doc = nlp_hu(' '.join(s[1]))\n",
    "            nps = []\n",
    "            for ent in doc:\n",
    "                if not ent.is_stop:\n",
    "                    nps.append(ent.lemma_)\n",
    "            if not nps:\n",
    "                stop_count_hu += 1\n",
    "    if stop_count_en < 4 and stop_count_hu < 4:\n",
    "        stop_sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_vec(emb_path, nmax=50000):\n",
    "    vectors = []\n",
    "    word2id = {}\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            vectors.append(vect)\n",
    "            word2id[word] = len(word2id)\n",
    "            if len(word2id) == nmax:\n",
    "                break\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_path = '/home/adaamko/data/DMR/wiki.multi.en.vec'\n",
    "tgt_path = '/home/adaamko/data/DMR/wiki.multi.hu.vec'\n",
    "nmax = 250000  # maximum number of word embeddings to load\n",
    "\n",
    "src_embeddings, src_id2word, src_word2id = load_vec(src_path, nmax)\n",
    "tgt_embeddings, tgt_id2word, tgt_word2id = load_vec(tgt_path, nmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_word2id = {v: k for k, v in src_id2word.items()}\n",
    "tgt_word2id = {v: k for k, v in tgt_id2word.items()}\n",
    "\n",
    "def get_distance(src_word, tgt_word, src_emb, tgt_emb):\n",
    "    src_word_emb = src_emb[src_word2id[src_word]]\n",
    "    tgt_word_emb = tgt_emb[tgt_word2id[tgt_word]]\n",
    "    score = (tgt_word_emb / np.linalg.norm(tgt_word_emb)).dot(src_word_emb / np.linalg.norm(src_word_emb))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3969420479092985"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_word = \"instrument\"\n",
    "tgt_word = \"készülék\"\n",
    "\n",
    "get_distance(src_word, tgt_word, src_embeddings, tgt_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_nps(nps, language):\n",
    "    lemmas = []\n",
    "    if language == \"hu\":\n",
    "        #words = [word for word in nps if word.isupper() == False]\n",
    "        #words = [word.lower() for word in words if word.lower() not in stopwords.words('hungarian')]\n",
    "        words = [word for word in nps if word not in stopwords.words('hungarian')]\n",
    "        if not words:\n",
    "            words = nps\n",
    "        for np in words:\n",
    "            try:\n",
    "                if len(lem_hu.stem(np)) > 0:\n",
    "                    lemma = lem_hu.stem(np)[0][0]\n",
    "                    lemmas.append(lemma)\n",
    "                    lemmas.append(np)\n",
    "                else:\n",
    "                    lemmas.append(np)\n",
    "            except (IndexError, NameError) as e:\n",
    "                lemmas.append(np)\n",
    "        \n",
    "    if language == \"en\":\n",
    "        #words = [word.lower() for word in nps if word.lower() not in stopwords.words('english')]\n",
    "        words = [word for word in nps if word not in stopwords.words('english')]\n",
    "        if not words:\n",
    "            words = nps\n",
    "        for np in words:\n",
    "            if len(nlp_en(np)) > 0:\n",
    "                lemma = nlp_en(np)[0].lemma_\n",
    "            else:\n",
    "                lemma = np\n",
    "            if lemma == \"-PRON-\":\n",
    "                lemmas.append(np.lower())\n",
    "            else:\n",
    "                #lemmas.append(lemma.lower())\n",
    "                lemmas.append(lemma)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def return_morph(word):\n",
    "    s = lem_hu.analyze(word)\n",
    "    ret_list = []\n",
    "    for i in s:\n",
    "        i = i.split(\"=\")\n",
    "        morpheme = i[1]\n",
    "        morpheme = morpheme.split(\"+\")\n",
    "        for m in morpheme:\n",
    "            mor = m.split(\"[\")[0].strip()\n",
    "            if len(mor) > 2:\n",
    "                ret_list.append(m.split(\"[\")[0].strip())\n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def count_vowels(word):\n",
    "    c = {v:word.count(v) for v in 'aeuioáéúüűíóöő'}\n",
    "    count = sum(c.values())\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embedding_score(en_np, hu_np):\n",
    "    max_score = 0\n",
    "    ancestors_word_en = []\n",
    "    for word in en_np:\n",
    "        w = word.strip(\"-\").lower()\n",
    "        if w in en_ancestors:\n",
    "            for anc in en_ancestors[w]:\n",
    "                if anc not in en_ancestor_top:\n",
    "                    ancestors_word_en.append(anc)\n",
    "        ancestors_word_en.append(w)\n",
    "    \n",
    "    ancestors_word_hu = []\n",
    "    for hu_word in hu_np:\n",
    "        if hu_word in hu_ancestors:\n",
    "            for anc in hu_ancestors[hu_word]:\n",
    "                if anc not in hu_ancestor_top:\n",
    "                    ancestors_word_hu.append(anc)\n",
    "        ancestors_word_hu.append(hu_word)\n",
    "        \n",
    "    for word in ancestors_word_en:\n",
    "        for hu_word in ancestors_word_hu:\n",
    "            try:\n",
    "                distance = get_distance(word, hu_word, src_embeddings, tgt_embeddings)\n",
    "            except KeyError:\n",
    "                distance = 0\n",
    "            if distance > max_score:\n",
    "                max_score = distance\n",
    "    return max_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_scores(sen):\n",
    "    en_nps = {}\n",
    "    hu_nps = {}\n",
    "    for s in sen['en_sen']:\n",
    "        if type(s) == tuple:\n",
    "            np_to_filter = s[1]\n",
    "            if not np_to_filter:\n",
    "                for np in sen['en_sen']:\n",
    "                    if type(np) == tuple:\n",
    "                        if np[0] == s[0]:\n",
    "                            np_to_filter = np[1]\n",
    "            lemmas = filter_nps(np_to_filter, \"en\")\n",
    "            en_nps[s[0]] = lemmas\n",
    "            \n",
    "    for s in sen['hu_sen']:\n",
    "        if type(s) == tuple:\n",
    "            np_to_filter = s[1]\n",
    "            if not np_to_filter:\n",
    "                for np in sen['hu_sen']:\n",
    "                    if type(np) == tuple:\n",
    "                        if np[0] == s[0]:\n",
    "                            np_to_filter = np[1]\n",
    "            lemmas = filter_nps(np_to_filter, \"hu\")\n",
    "            hu_nps[s[0]] = lemmas\n",
    "    \n",
    "            \n",
    "    scores = [[] for i in range(len(en_nps))]\n",
    "    embedding_scores = [[] for i in range(len(en_nps))]\n",
    "    \n",
    "    for en_np in en_nps:\n",
    "        for hu_np in hu_nps:\n",
    "            l = []\n",
    "            hu_lower = [s.lower() for s in hu_nps[hu_np]]\n",
    "            \n",
    "            #add_morphs = []\n",
    "            #for low in hu_lower:\n",
    "            #    if count_vowels(low) > 3:\n",
    "            #        ms = return_morph(low)\n",
    "            #        add_morphs += ms\n",
    "            #for addit in add_morphs:\n",
    "            #    if addit not in hu_lower:\n",
    "            #        hu_lower.append(addit)\n",
    "            \n",
    "            for word in en_nps[en_np]:\n",
    "                dic_elements = []\n",
    "                w = word.strip(\"-\")\n",
    "                if not dictionary[w]:\n",
    "                    dic_elements.append(w)\n",
    "                for el in dictionary[w]:\n",
    "                     #for i in el.split():\n",
    "                    #process_el = filter_nps([el], \"hu\")\n",
    "                    dic_elements.append(el)\n",
    "                inter = []\n",
    "                for en_word in dic_elements:\n",
    "                    for hu_word in hu_lower:\n",
    "                        dis = distance(en_word, hu_word)\n",
    "                        \n",
    "                        if(len(en_word) > 5 or len(hu_word) > 5):\n",
    "                            if dis < 3:\n",
    "                                inter.append(True)\n",
    "                        else:\n",
    "                            if dis < 1:\n",
    "                                inter.append(True)\n",
    "                                \n",
    "                l.append(len(inter) > 0)\n",
    "            listmax = max([hu_lower, en_nps[en_np]], key=len)\n",
    "            if len(listmax) == 0:\n",
    "                score = 0\n",
    "            else:\n",
    "                score = float(l.count(True)/len(listmax))\n",
    "            scores[en_np].append(score)\n",
    "            \n",
    "            emb_score = get_embedding_score(en_nps[en_np], hu_lower)\n",
    "            embedding_scores[en_np].append(emb_score)\n",
    "    return [scores, embedding_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process(sen):\n",
    "    scores = compute_scores(sen)\n",
    "    if scores[0] is None:\n",
    "        return None\n",
    "    aligns = []\n",
    "    for i in range(len(scores[0])):\n",
    "        for j,k in enumerate(scores[0][i]):\n",
    "            #if (float(k) > 0.01) or float(scores[1][i][j]) > 0.5:\n",
    "            if float(scores[1][i][j]) >= 0.5:\n",
    "            #and float(scores[1][i][j]) > 0.1:\n",
    "                aligns.append((str(i), str(j)))\n",
    "    return aligns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "guesses = []\n",
    "senaligns = {}\n",
    "for i,sentence in enumerate(sentences):\n",
    "    print(i)\n",
    "    gold = sentence['aligns']\n",
    "    gold_filtered = []\n",
    "    for goldalign in gold:\n",
    "        en = re.findall('\\d+', goldalign[0] )\n",
    "        hu = re.findall('\\d+', goldalign[1] )\n",
    "        gold_filtered.append((str(en[0]), str(hu[0])))\n",
    "    al = process(sentence)\n",
    "    senaligns[sentence['id']] = al\n",
    "    if al is not None:        \n",
    "        for i in al:\n",
    "            if i in gold_filtered:\n",
    "                guesses.append(True)\n",
    "            else:\n",
    "                guesses.append(False)\n",
    "        #for g in gold_filtered:\n",
    "        #    if g not in al:\n",
    "        #        print(g)\n",
    "        #        print(sentence['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6561523495076941\n",
      "18848\n",
      "16961\n",
      "0.5904605263157895\n",
      "0.621575581557709\n"
     ]
    }
   ],
   "source": [
    "score = float(guesses.count(True)/len(guesses))\n",
    "np_len = 0\n",
    "for sen in sentences:\n",
    "    np_len += len(sen['aligns'])\n",
    "print(score)\n",
    "print(np_len)\n",
    "print(len(guesses))\n",
    "recall = (score * len(guesses)) / np_len\n",
    "f1_score = (2*recall*score)/(recall+score)\n",
    "print(recall)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.7006302521008403\n",
    "2506\n",
    "2856\n",
    "0.798483639265762\n",
    "0.7463632972771354"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only morph and dict\n",
    "0.768337505992649\n",
    "18848\n",
    "18773\n",
    "0.7652801358234296\n",
    "0.7668057733712554"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embedding and morph 0,6\n",
    "0.7508176923463996\n",
    "18848\n",
    "19873\n",
    "0.7916489813242784\n",
    "0.7706929056584283"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embedding and morph 0,6 on stop filtered\n",
    "0.8114015176081458\n",
    "16146\n",
    "15419\n",
    "0.7748668400842313\n",
    "0.7927134484397276"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embedding+4lang + morph 0,6 0.7417228317016775\n",
    "18848\n",
    "20327\n",
    "0.7999257215619694\n",
    "0.7697255902999361"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "with open('hu_ancestors.json') as f:\n",
    "    hu_ancestors = json.load(f)\n",
    "with open('en_ancestors.json') as f:\n",
    "    en_ancestors = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "en_ancestors_filtered = {}\n",
    "for ancestor in en_ancestors:\n",
    "    anc_list = en_ancestors[ancestor]\n",
    "    anc = ancestor.strip(\"-\")\n",
    "    en_ancestors_filtered[anc] = anc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_ancestors = en_ancestors_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ancestor_count = defaultdict(int)\n",
    "for ancestor in en_ancestors:\n",
    "    for a in en_ancestors[ancestor]:\n",
    "        ancestor_count[a] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_by_value = sorted(ancestor_count.items(), key=lambda kv: kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_ancestor_top = sorted_by_value[-500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_ancestor_top = [word[0] for word in en_ancestor_top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ancestor_count = defaultdict(int)\n",
    "for ancestor in hu_ancestors:\n",
    "    for a in hu_ancestors[ancestor]:\n",
    "        ancestor_count[a] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_by_value = sorted(ancestor_count.items(), key=lambda kv: kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hu_ancestor_top = sorted_by_value[-500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hu_ancestor_top = [word[0] for word in hu_ancestor_top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_by_value[-500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "guesses_stop = []\n",
    "senaligns = {}\n",
    "for i,sentence in enumerate(stop_sentences):\n",
    "    print(i)\n",
    "    gold = sentence['aligns']\n",
    "    gold_filtered = []\n",
    "    for goldalign in gold:\n",
    "        en = re.findall('\\d+', goldalign[0] )\n",
    "        hu = re.findall('\\d+', goldalign[1] )\n",
    "        gold_filtered.append((str(en[0]), str(hu[0])))\n",
    "    al = process(sentence)\n",
    "    senaligns[sentence['id']] = al\n",
    "    if al is not None:        \n",
    "        for i in al:\n",
    "            if i in gold_filtered:\n",
    "                guesses_stop.append(True)\n",
    "            else:\n",
    "                guesses_stop.append(False)\n",
    "        #for g in gold_filtered:\n",
    "        #    if g not in al:\n",
    "        #        print(g)\n",
    "        #        print(sentence['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hu_ancestor_top_filtered = []\n",
    "en_ancestor_top_filtered = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('en_ancestor_top', 'r') as f:\n",
    "    for item in f:\n",
    "        en_ancestor_top_filtered.append(item.strip(\"\\n\"))\n",
    "        \n",
    "with open('hu_ancestor_top', 'r') as f:\n",
    "    for item in f:\n",
    "        hu_ancestor_top_filtered.append(item.strip(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
