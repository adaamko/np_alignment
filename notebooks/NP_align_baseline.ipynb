{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for processing the glove file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "class InvalidBracketsException(Exception):\n",
    "    pass\n",
    "\n",
    "class InvalidNumberingException(Exception):\n",
    "    pass\n",
    "\n",
    "class InvalidAlignException(Exception):\n",
    "    pass\n",
    "\n",
    "def process_sen(sen):\n",
    "    toks = []\n",
    "    state = \"normal\"\n",
    "    counter = 0\n",
    "    for tok in sen.split():\n",
    "        # checking valid brackets\n",
    "        validate_brackets(tok)\n",
    "        \n",
    "        # processing\n",
    "        if state == \"normal\":\n",
    "            if tok == \"[\":\n",
    "                state = \"inside\"\n",
    "                actual_np = []\n",
    "                continue\n",
    "            else:\n",
    "                toks.append(tok)\n",
    "                continue\n",
    "        elif state == \"inside\":\n",
    "            if tok == \"]\":\n",
    "                state = \"after\"\n",
    "                continue\n",
    "            else:\n",
    "                actual_np.append(tok)\n",
    "                continue\n",
    "        elif state == \"after\":\n",
    "            index = int(tok)\n",
    "            # checking valid index ordering\n",
    "            if index != counter:\n",
    "                raise InvalidNumberingException\n",
    "            \n",
    "            counter += 1\n",
    "            \n",
    "            toks.append((index, actual_np))\n",
    "            actual_np = []\n",
    "            state = \"normal\"\n",
    "            continue\n",
    "    \n",
    "    return toks\n",
    "    \n",
    "def validate_brackets(t):\n",
    "    opener = t.find(\"[\")\n",
    "    closer = t.find(\"]\")\n",
    "    if opener > -1 or closer > -1:\n",
    "        if len(t) > 1:\n",
    "            sys.stderr.write(\"token: %s\\n\" % t)\n",
    "            raise InvalidBracketsException\n",
    "    \n",
    "\n",
    "def extract_np_tok_indices(sen):\n",
    "    indices = {}\n",
    "    actual_index = 0\n",
    "    for tok in sen:\n",
    "        if isinstance(tok, tuple):\n",
    "            index = tok[0]\n",
    "            indices[index] = []\n",
    "            for np_tok in tok[1]:\n",
    "                indices[index].append(actual_index)\n",
    "                actual_index += 1\n",
    "        else:\n",
    "            actual_index += 1\n",
    "    return indices\n",
    "\n",
    "c = u'\\u2015'    \n",
    "def process_aligns(als):\n",
    "    aligns = []\n",
    "    for align in als.split():\n",
    "        try:\n",
    "            if align.find(c) > -1:\n",
    "                en = align.split(c)[0]\n",
    "                hu = align.split(c)[1]\n",
    "            else:\n",
    "                en = align.split(\"-\")[0]\n",
    "                hu = align.split(\"-\")[1]\n",
    "        except IndexError:\n",
    "            sys.stderr.write(\"%s\\n\" % als)\n",
    "            raise Exception(\"Malformed aligned file\")\n",
    "        \n",
    "        aligns.append((en, hu))\n",
    "    return aligns\n",
    "  \n",
    "def validate_aligns(sen):\n",
    "    en_sen = sen[\"en_sen\"]\n",
    "    hu_sen = sen[\"hu_sen\"]\n",
    "    aligns = sen[\"aligns\"]\n",
    "    \n",
    "    en_np_indices = set([tok[0] for tok in en_sen if isinstance(tok, tuple)])\n",
    "    hu_np_indices = set([tok[0] for tok in hu_sen if isinstance(tok, tuple)])\n",
    "    for align in aligns:\n",
    "        en_i = int(align[0].strip(\"sb\"))\n",
    "        hu_i = int(align[1].strip(\"sb\"))\n",
    "        if  not ( ( en_i in en_np_indices ) and ( hu_i in hu_np_indices ) ) :\n",
    "            sys.stderr.write(\"%d-%d\\n\" % (en_i, hu_i) )\n",
    "            raise InvalidAlignException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in the file, storing the sentences in a list of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "empty_sentence = {\n",
    "    'id': None,\n",
    "    'en_sen': None,\n",
    "    'hu_sen': None,\n",
    "    'aligns': None\n",
    "}\n",
    "sentences = []\n",
    "\n",
    "actual_sentence = dict(empty_sentence)\n",
    "state = \"empty\"\n",
    "with open(\"/home/adaamko/data/1500-test.txt\") as runga_input_file:\n",
    "    for line in runga_input_file:\n",
    "        if state == \"empty\":\n",
    "            try:\n",
    "                actual_sentence[\"id\"] = int(line.strip())\n",
    "            except ValueError:\n",
    "                # reached end of file or malformed input\n",
    "                continue\n",
    "            state = \"got_id\"\n",
    "            continue\n",
    "        elif state == \"got_id\":\n",
    "            try:\n",
    "                actual_sentence[\"en_sen\"] = process_sen(line.strip())\n",
    "            except InvalidBracketsException:\n",
    "                raise InvalidBracketsException(\"Invalid English bracketing in sentence: %d\\n\" % actual_sentence[\"id\"])\n",
    "            except InvalidNumberingException:\n",
    "                raise InvalidNumberingException(\"Invalid English np numbering in sentence: %d\\n\" % actual_sentence[\"id\"])\n",
    "            except:\n",
    "                raise Exception(\"Unknown error in english sentence: %d\\n\" % actual_sentence[\"id\"])\n",
    "            state = \"got_en\"\n",
    "        elif state == \"got_en\":\n",
    "            try:\n",
    "                assert(line.strip() == \"\")\n",
    "            except AssertionError:\n",
    "                print(line)\n",
    "                raise Exception(\"MyOwn\")\n",
    "            state = \"wait_for_hu\"\n",
    "            continue\n",
    "        elif state == \"wait_for_hu\":\n",
    "            try:\n",
    "                actual_sentence[\"hu_sen\"] = process_sen(line.strip())\n",
    "            except InvalidBracketsException:\n",
    "                raise InvalidBracketsException(\"Invalid Hungarian bracketing in sentence: %d\\n\" % actual_sentence[\"id\"])\n",
    "            except InvalidNumberingException:\n",
    "                raise InvalidNumberingException(\"Invalid Hungarian np numbering in sentence: %d\\n\" % actual_sentence[\"id\"])\n",
    "            state = \"got_hu\"\n",
    "            continue\n",
    "        elif state == \"got_hu\":\n",
    "            assert(line.strip() == \"\")\n",
    "            state = \"wait_for_aligns\"\n",
    "            continue\n",
    "        elif state == \"wait_for_aligns\":\n",
    "            actual_sentence[\"aligns\"] = process_aligns(line.strip())\n",
    "            try:\n",
    "                validate_aligns(actual_sentence)\n",
    "            except InvalidAlignException:\n",
    "                raise InvalidAlignException(\"Invalid np alignment in sentence: %d\\n\" % actual_sentence[\"id\"])\n",
    "            state = \"got_align\"\n",
    "            continue\n",
    "        elif state == \"got_align\":\n",
    "            try:\n",
    "                assert(line.strip() == \"\")\n",
    "            except AssertionError:\n",
    "                sys.stderr.write(\"%d\\n\" % actual_sentence[\"id\"])\n",
    "                raise Exception(\"Malformed input file\")\n",
    "            state = \"wait_for_last_but_on_line\"\n",
    "            continue\n",
    "        elif state == \"wait_for_last_but_on_line\":\n",
    "            assert(line.strip() == \"\")\n",
    "            state = \"wait_for_last_line\"\n",
    "            continue\n",
    "        elif state == \"wait_for_last_line\":\n",
    "            if not line.strip() == \"\":\n",
    "                sys.stderr.write(\"Missing last empty line after sentence: %d\\n\" % actual_sentence[\"id\"])\n",
    "                continue\n",
    "            sentences.append(actual_sentence)\n",
    "            actual_sentence = dict(empty_sentence)\n",
    "            state = \"empty\"\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in an english-hungarian dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'sblood\", 'O', 'a_kutya_teremte1sit']\n",
      "[\"'sdeath\", 'O', 'az_ista1llo1ja1t!']\n",
      "[\"'shun\", 'O', 'attention!']\n",
      "[\"'shun\", 'O', 'vigya1zz!']\n",
      "[\"'tis\", 'N', 'it_is']\n",
      "[\"'tween-decks\", 'N', 'fede1lko2z']\n",
      "['-featured', 'A', '-arcu1']\n",
      "['-featured', 'A', '-vona1su1']\n",
      "['-fold', 'D', '-szeres(en)']\n",
      "['-fold', 'D', '-szoros(an)']\n",
      "['-haired', 'A', '-haju1']\n",
      "['-haired', 'A', '-szo3ru3']\n",
      "['-lived', 'A', '-e1letu3']\n",
      "['-necked', 'N', 'cso1kolo1dza1s']\n",
      "['-necked', 'N', 'o2lelkeze1s']\n",
      "['-necked', 'N', 'oszlopnyak']\n",
      "['-nosed', 'A', 'orru1']\n",
      "['-oared', 'A', 'evezo3s']\n",
      "['-ology', 'N', '-tudoma1ny']\n",
      "['-paced', 'A', '-ja1ra1su1']\n",
      "['-paced', 'A', '-le1ptu3']\n",
      "['-roomed', 'A', '-szoba1s']\n",
      "['-seater', 'N', '-u2le1su3']\n",
      "['-sided', 'A', '-oldalu1']\n",
      "['-sidedness', 'N', '-oldalu1sa1g']\n",
      "['-sighted', 'A', '-la1ta1su1']\n",
      "['-sighted', 'A', '-la1to1']\n",
      "['-skulled', 'A', 'koponya1ju1']\n",
      "['-sleeved', 'A', '-ujjas']\n",
      "['-sleeved', 'A', '-ujju1']\n",
      "['-tongued', 'A', '-hangu1']\n",
      "['-tongued', 'A', '-nyelvu3']\n",
      "['-tongued', 'A', '-szavu1']\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "dictionary = defaultdict(list)\n",
    "count=0\n",
    "with open(\"/home/adaamko/data/hokoto\", errors=\"replace\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip().split(\"@\")\n",
    "        if count<33:\n",
    "            print(line)\n",
    "        count+=1\n",
    "        dictionary[line[0]].append(line[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacing characters to utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary_filtered = defaultdict(list)\n",
    "old_char = [\"a1\", \"e1\", \"u1\", \"i1\", \"o1\", \"A1\", \"E1\", \"U1\", \"I1\", \"O1\", \"o2\", \"u2\", \"O2\", \"U2\", \"o3\", \"u3\", \"O3\", \"U3\", \"_\"]\n",
    "new_char = [\"á\", \"é\", \"ú\", \"í\", \"ó\", \"Á\", \"É\", \"Ú\", \"Í\", \"Ó\", \"ö\", \"ü\", \"Ö\", \"Ü\", \"ő\", \"ű\", \"Ő\", \"Ű\", \" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in dictionary:\n",
    "    for j in range(len(dictionary[i])):\n",
    "        for k in range(len(old_char)):\n",
    "            dictionary[i][j] = dictionary[i][j].replace(old_char[k], new_char[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can start building the baseline method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pprint\n",
    "def compute_scores(sen, lem_en, lem_hu, dic):\n",
    "    en_nps = {}\n",
    "    hu_nps = {}\n",
    "    for s in sen['en_sen']:\n",
    "        if type(s) == tuple:\n",
    "            lemmas = []\n",
    "            for np in s[1]:\n",
    "                lemma = lem_en(np)[0].lemma_\n",
    "                if lemma == \"-PRON-\":\n",
    "                    lemmas.append(np.lower())\n",
    "                else:\n",
    "                    lemmas.append(lemma)\n",
    "            en_nps[s[0]] = lemmas\n",
    "    for s in sen['hu_sen']:\n",
    "        if type(s) == tuple:\n",
    "            lemmas = []\n",
    "            for np in s[1]:\n",
    "                try:\n",
    "                    lemmas.append(lem_hu.stem(np)[0][0])\n",
    "                except IndexError:\n",
    "                    print(\"indexerror\")\n",
    "                    return None\n",
    "            hu_nps[s[0]] = lemmas\n",
    "    scores = [[] for i in range(len(en_nps))]\n",
    "    \n",
    "    print(en_nps)\n",
    "    print(hu_nps)\n",
    "    dic_elements = defaultdict(list)\n",
    "    for en_np in en_nps:\n",
    "        for word in en_nps[en_np]:\n",
    "            for el in dictionary[word]:\n",
    "                dic_elements[word].append(el)\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    pp.pprint(dic_elements)\n",
    "    for en_np in en_nps:\n",
    "        for hu_np in hu_nps:\n",
    "            l = []\n",
    "            hu_lower = [s.lower() for s in hu_nps[hu_np]]\n",
    "            for word in en_nps[en_np]:\n",
    "                dic_elements = []\n",
    "                for el in dictionary[word]:\n",
    "                    if len(el.split()) > 1 and el.split()[1].startswith(\"(\"):\n",
    "                        for x in lem_hu.stem(el.split()[0]):\n",
    "                                dic_elements.append(x[0])\n",
    "                    elif len(el.split()) >= 1 and el.split()[0].startswith(\"(\"):\n",
    "                        for x in lem_hu.stem(el.split(\")\")[1].strip()):\n",
    "                                dic_elements.append(x[0])\n",
    "                    else:\n",
    "                        for i in el.split():\n",
    "                            if len(lem_hu.stem(i)) > 0:\n",
    "                                for x in lem_hu.stem(i):\n",
    "                                    dic_elements.append(x[0])\n",
    "                inter = list(set(hu_lower) & set(dic_elements))\n",
    "                l.append(len(inter) > 0)\n",
    "            listmax = max([hu_lower, en_nps[en_np]], key=len)\n",
    "            score = float(l.count(True)/len(listmax))\n",
    "            scores[en_np].append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import emmorphpy.emmorphpy as emmorph\n",
    "import itertools\n",
    "from itertools import permutations, repeat\n",
    "\n",
    "nlp_en = spacy.load('en')\n",
    "m = emmorph.EmMorphPy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences_align = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def align_sentence(sen):\n",
    "    scores = compute_scores(sen, nlp_en, m, dictionary)\n",
    "    en_np_score = [x for x in range(len(scores))]\n",
    "    hu_np_score = [x for x in range(len(scores[0]))]\n",
    "    print(en_np_score)\n",
    "    print(hu_np_score)\n",
    "    a=[(x,y) for x in en_np_score for y in hu_np_score]\n",
    "    permutations_aligned = []\n",
    "    if len(hu_np_score) >= len(en_np_score):\n",
    "        permut = list(list(zip(r, p)) for (r, p) in zip(repeat(en_np_score), permutations(hu_np_score)))\n",
    "        max_score = []\n",
    "        for perm in permut:\n",
    "            max_score.append(sum([scores[x[0]][x[1]] for x in perm]))\n",
    "        permutations_aligned.extend(permut[max_score.index(max(max_score))])\n",
    "    else:\n",
    "        permut = list(list(zip(r, p)) for (r, p) in zip(repeat(hu_np_score), permutations(en_np_score)))\n",
    "        max_score = []\n",
    "        for perm in permut:\n",
    "            max_score.append(sum([scores[x[1]][x[0]] for x in perm]))\n",
    "        aligns = [(x[1], x[0]) for x in permut[max_score.index(max(max_score))]]\n",
    "        permutations_aligned.extend(aligns)\n",
    "    return permutations_aligned\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aligns': [('1', '0'), ('2', '1'), ('3', '2')],\n",
       " 'en_sen': [(0, ['It']),\n",
       "  'was',\n",
       "  (1, ['a', 'bright', 'cold', 'day', 'in', 'April']),\n",
       "  ',',\n",
       "  'and',\n",
       "  (2, ['the', 'clocks']),\n",
       "  'were',\n",
       "  'striking',\n",
       "  (3, ['thirteen']),\n",
       "  '.'],\n",
       " 'hu_sen': [(0, ['Derült', ',', 'hideg', 'áprilisi', 'nap']),\n",
       "  'volt',\n",
       "  ',',\n",
       "  (1, ['az', 'órák']),\n",
       "  'éppen',\n",
       "  (2, ['tizenhármat']),\n",
       "  'ütöttek',\n",
       "  '.'],\n",
       " 'id': 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def align_sentence_asmax(sen):\n",
    "    scores = compute_scores(sen, nlp_en, m, dictionary)\n",
    "    if scores is None:\n",
    "        return None\n",
    "    aligns = []\n",
    "    for i in range(len(scores)):\n",
    "        for j,k in enumerate(scores[i]):\n",
    "            if float(k) > 0.6:\n",
    "                aligns.append((str(i), str(j)))\n",
    "    return aligns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ['he'], 1: ['some', 'childhood', 'memory'], 2: ['that'], 3: ['him'], 4: ['london'], 5: ['this']}\n",
      "{0: ['maga'], 1: ['valami', 'gyermekkori', 'emlék'], 2: ['amely'], 3: ['neki'], 4: ['ilyen'], 5: ['London']}\n",
      "defaultdict(<class 'list'>,\n",
      "            {   'childhood': ['gyerekkor', 'gyermekkor'],\n",
      "                'he': ['férfi', 'hím (állat)', 'hímnemű személy', 'ő'],\n",
      "                'him': ['őt'],\n",
      "                'memory': ['emlékezőtehetség', 'memória'],\n",
      "                'some': [   'bizonyos',\n",
      "                            'egész',\n",
      "                            'egy bizonyos',\n",
      "                            'egy kevés',\n",
      "                            'egy kis',\n",
      "                            'egyes',\n",
      "                            'igazi',\n",
      "                            'körülbelül',\n",
      "                            'komoly',\n",
      "                            'meglehetősen',\n",
      "                            'néhány',\n",
      "                            'némely',\n",
      "                            'némi',\n",
      "                            'pompás',\n",
      "                            'több',\n",
      "                            'valamelyik',\n",
      "                            'valami',\n",
      "                            '2meglehetősen',\n",
      "                            '2némileg',\n",
      "                            'körülbelül',\n",
      "                            'mintegy',\n",
      "                            'némileg',\n",
      "                            'valami',\n",
      "                            'néhány',\n",
      "                            'némely'],\n",
      "                'that': [   'aki',\n",
      "                            'akit',\n",
      "                            'amaz',\n",
      "                            'ami',\n",
      "                            'amit',\n",
      "                            'az',\n",
      "                            'azért, hogy',\n",
      "                            'hogy',\n",
      "                            'ilyen',\n",
      "                            'olyan',\n",
      "                            'annyira',\n",
      "                            'ennyire',\n",
      "                            'evégből',\n",
      "                            'aki(t)',\n",
      "                            'amaz(t)',\n",
      "                            'amely(et)',\n",
      "                            'ami(t)',\n",
      "                            'az(t)',\n",
      "                            'azért',\n",
      "                            'hogy'],\n",
      "                'this': [   'ilyen',\n",
      "                            'ennyira',\n",
      "                            'aki(t)',\n",
      "                            'amely(et)',\n",
      "                            'ami(t)',\n",
      "                            'emez(t)',\n",
      "                            'ez(t)']})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('2', '4'), ('5', '4')]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = align_sentence_asmax(sentences[40])\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indexerror\n",
      "indexerror\n",
      "indexerror\n",
      "indexerror\n",
      "indexerror\n",
      "indexerror\n",
      "indexerror\n",
      "indexerror\n",
      "indexerror\n",
      "indexerror\n",
      "indexerror\n",
      "indexerror\n",
      "indexerror\n",
      "indexerror\n",
      "indexerror\n",
      "indexerror\n",
      "indexerror\n",
      "indexerror\n",
      "indexerror\n",
      "indexerror\n",
      "indexerror\n",
      "indexerror\n",
      "indexerror\n",
      "indexerror\n",
      "indexerror\n",
      "indexerror\n",
      "indexerror\n",
      "indexerror\n",
      "indexerror\n",
      "indexerror\n",
      "indexerror\n",
      "indexerror\n",
      "indexerror\n"
     ]
    }
   ],
   "source": [
    "guesses = []\n",
    "senaligns = {}\n",
    "for sentence in sentences:\n",
    "    gold = sentence['aligns']\n",
    "    gold_filtered = []\n",
    "    for goldalign in gold:\n",
    "        en = re.findall('\\d+', goldalign[0] )\n",
    "        hu = re.findall('\\d+', goldalign[1] )\n",
    "        gold_filtered.append((str(en[0]), str(hu[0])))\n",
    "    al = align_sentence_asmax(sentence)\n",
    "    senaligns[sentence['id']] = al\n",
    "    if al is not None:        \n",
    "        for i in al:\n",
    "            if i in gold_filtered:\n",
    "                guesses.append(True)\n",
    "            else:\n",
    "                guesses.append(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6263473053892216"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = float(guesses.count(True)/len(guesses))\n",
    "np_len = 0\n",
    "for sen in sentences:\n",
    "    np_len += len(sen['aligns'])\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "835"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(guesses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senaligns[40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(50):\n",
    "    try:\n",
    "        sent = align_sentence(sentences[i])\n",
    "    except:\n",
    "        continue\n",
    "    sentences_align[sentences[i]['id']] = sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {0: [(1, 0), (0, 1), (3, 2)],\n",
       "             1: [(0, 0), (1, 1), (4, 2), (3, 3), (2, 4), (5, 5)],\n",
       "             2: [(0, 0), (1, 1)],\n",
       "             3: [(0, 0), (1, 1), (2, 2)],\n",
       "             4: [(0, 1), (1, 0), (2, 4)],\n",
       "             5: [(0, 0), (1, 1)],\n",
       "             6: [(0, 1), (1, 0)],\n",
       "             7: [(0, 0), (1, 1), (4, 2)],\n",
       "             8: [(0, 0), (1, 1), (2, 4)],\n",
       "             9: [(1, 0),\n",
       "              (0, 1),\n",
       "              (2, 2),\n",
       "              (3, 3),\n",
       "              (7, 4),\n",
       "              (5, 5),\n",
       "              (6, 6),\n",
       "              (4, 7)],\n",
       "             10: [(0, 1), (1, 2), (2, 3), (3, 4)],\n",
       "             11: [(0, 1), (1, 2), (2, 3), (3, 0)],\n",
       "             12: [(0, 0), (1, 1), (2, 2)],\n",
       "             13: [(0, 0), (1, 1), (2, 4), (3, 3), (4, 5), (5, 2)],\n",
       "             14: [(0, 0), (1, 1), (2, 2), (3, 4)],\n",
       "             15: [(0, 0), (1, 1), (2, 2), (3, 3), (4, 5)],\n",
       "             16: [(0, 0), (1, 1)],\n",
       "             17: [(0, 0), (1, 5), (2, 2), (3, 1), (4, 6)],\n",
       "             18: [(0, 0), (1, 2), (2, 5), (3, 6), (4, 7), (5, 1)],\n",
       "             19: [(0, 0), (1, 2)]})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0), (0, 1), (3, 2)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
