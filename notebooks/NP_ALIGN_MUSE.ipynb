{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import matplotlib\n",
    "import re\n",
    "import pydot\n",
    "from graphviz import Digraph\n",
    "from graphviz import Source\n",
    "from baseline_utils import process_baseline\n",
    "from nltk.corpus import stopwords\n",
    "import pprint\n",
    "from Levenshtein import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6567"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = process_baseline(\"/home/adaamko/data/1984.sen-aligned.np-aligned.gold\")\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import hu_core_ud_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import emmorphpy.emmorphpy as emmorph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lem_hu = emmorph.EmMorphPy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp_hu = hu_core_ud_lg.load()\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import json\n",
    "import request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_vec(emb_path, nmax=50000):\n",
    "    vectors = []\n",
    "    word2id = {}\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            vectors.append(vect)\n",
    "            word2id[word] = len(word2id)\n",
    "            if len(word2id) == nmax:\n",
    "                break\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "src_path = '/home/adaamko/data/DMR/wiki.multi.en.vec'\n",
    "tgt_path = '/home/adaamko/data/DMR/wiki.multi.hu.vec'\n",
    "nmax = 250000  # maximum number of word embeddings to load\n",
    "\n",
    "src_embeddings, src_id2word, src_word2id = load_vec(src_path, nmax)\n",
    "tgt_embeddings, tgt_id2word, tgt_word2id = load_vec(tgt_path, nmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_nn(word, src_emb, src_id2word, tgt_emb, tgt_id2word, K=5):\n",
    "    word2id = {v: k for k, v in src_id2word.items()}\n",
    "    word_emb = src_emb[word2id[word]]\n",
    "    scores = (tgt_emb / np.linalg.norm(tgt_emb, 2, 1)[:, None]).dot(word_emb / np.linalg.norm(word_emb))\n",
    "    k_best = scores.argsort()[-K:][::-1]\n",
    "    for i, idx in enumerate(k_best):\n",
    "        print('%.4f - %s' % (scores[idx], tgt_id2word[idx]))\n",
    "    words = []\n",
    "    for i, idx in enumerate(k_best):\n",
    "        words.append(tgt_id2word[idx])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_word2id = {v: k for k, v in src_id2word.items()}\n",
    "tgt_word2id = {v: k for k, v in tgt_id2word.items()}\n",
    "\n",
    "def get_distance(src_word, tgt_word, src_emb, tgt_emb):\n",
    "    src_word_emb = src_emb[src_word2id[src_word]]\n",
    "    tgt_word_emb = tgt_emb[tgt_word2id[tgt_word]]\n",
    "    score = (tgt_word_emb / np.linalg.norm(tgt_word_emb)).dot(src_word_emb / np.linalg.norm(src_word_emb))\n",
    "    return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Winston'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-e1c1b35c8e85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msrc_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Winston'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_id2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_id2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-090bcda0a96c>\u001b[0m in \u001b[0;36mget_nn\u001b[0;34m(word, src_emb, src_id2word, tgt_emb, tgt_id2word, K)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_id2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_id2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mword2id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msrc_id2word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mword_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc_emb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword2id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtgt_emb\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_emb\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mk_best\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Winston'"
     ]
    }
   ],
   "source": [
    "src_word = 'Winston'\n",
    "get_nn(src_word, src_embeddings, src_id2word, tgt_embeddings, tgt_id2word, K=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7415513806514814"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_word = \"thirteen\"\n",
    "tgt_word = \"tizenhárom\"\n",
    "\n",
    "get_distance(src_word, tgt_word, src_embeddings, tgt_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_nps(nps, language):\n",
    "    lemmas = []\n",
    "    if language == \"hu\":\n",
    "        doc = nlp_hu(nps.lower())\n",
    "        for ent in doc:\n",
    "            if not ent.is_stop:               \n",
    "                try:\n",
    "                    lemmas.append(lem_hu.stem(ent.lemma_)[0][0].lower())\n",
    "                    lemmas.append(ent.lower_)\n",
    "                except IndexError:\n",
    "                    lemmas.append(ent.lower_)\n",
    "        \n",
    "    elif language == \"en\":\n",
    "        doc = nlp_en(nps.lower())\n",
    "        \n",
    "        words = []\n",
    "\n",
    "        for ent in doc:\n",
    "            if not ent.is_stop:\n",
    "                if ent.lemma_ == \"-PRON-\":\n",
    "                    words.append(ent.lower_)\n",
    "                else:\n",
    "                    words.append(ent.lemma_.lower())\n",
    "                    words.append(ent.lower_)\n",
    "\n",
    "        if not words:\n",
    "            for ent in doc:\n",
    "                words.append(ent.lower_)\n",
    "\n",
    "        lemmas += words\n",
    "        \n",
    "    return list(set(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def return_morph(word):\n",
    "    s = lem_hu.analyze(word)\n",
    "    ret_list = []\n",
    "    for i in s:\n",
    "        i = i.split(\"=\")\n",
    "        morpheme = i[1]\n",
    "        morpheme = morpheme.split(\"+\")\n",
    "        for m in morpheme:\n",
    "            mor = m.split(\"[\")[0].strip()\n",
    "            if len(mor) > 2:\n",
    "                ret_list.append(m.split(\"[\")[0].strip())\n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_vowels(word):\n",
    "    c = {v:word.count(v) for v in 'aeuioáéúüűíóöő'}\n",
    "    count = sum(c.values())\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_min_distance_scores(sen):\n",
    "    en_nps = {}\n",
    "    hu_nps = {}\n",
    "    for s in sen['en_sen']:\n",
    "        if type(s) == tuple:\n",
    "            np_to_filter = s[1]\n",
    "            if not np_to_filter:\n",
    "                for np in sen['en_sen']:\n",
    "                    if type(np) == tuple:\n",
    "                        if np[0] == s[0]:\n",
    "                            np_to_filter = np[1]\n",
    "            lemmas = filter_nps(' '.join(np_to_filter), \"en\")\n",
    "            en_nps[s[0]] = lemmas\n",
    "            \n",
    "    for s in sen['hu_sen']:\n",
    "        if type(s) == tuple:\n",
    "            np_to_filter = s[1]\n",
    "            if not np_to_filter:\n",
    "                for np in sen['hu_sen']:\n",
    "                    if type(np) == tuple:\n",
    "                        if np[0] == s[0]:\n",
    "                            np_to_filter = np[1]\n",
    "            lemmas = filter_nps(' '.join(np_to_filter), \"hu\")\n",
    "            hu_nps[s[0]] = lemmas\n",
    "    \n",
    "    scores = [[] for i in range(len(en_nps))]\n",
    "    \n",
    "    for en_np in en_nps:\n",
    "        for hu_np in hu_nps:\n",
    "            hu_lower = [s.lower() for s in hu_nps[hu_np]]\n",
    "            add_morphs = []\n",
    "            \n",
    "            #for low in hu_lower:\n",
    "            #    if count_vowels(low) >= 3:\n",
    "             #       m = return_morph(low)\n",
    "                    #add_morphs += m\n",
    "                    \n",
    "            #for addit in add_morphs:\n",
    "             #   if addit not in hu_lower:\n",
    "                    #hu_lower.append(addit)\n",
    "                    \n",
    "            max_score = 0\n",
    "            for word in en_nps[en_np]:\n",
    "                w = word.strip(\"-\").lower()\n",
    "                for hu_word in hu_lower:\n",
    "                    try:\n",
    "                        distance = get_distance(w, hu_word, src_embeddings, tgt_embeddings)\n",
    "                    except KeyError:\n",
    "                        distance = 0\n",
    "                    if distance > max_score:\n",
    "                        max_score = distance\n",
    "            scores[en_np].append(max_score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_KNN_scores(sen):\n",
    "    en_nps = {}\n",
    "    hu_nps = {}\n",
    "    for s in sen['en_sen']:\n",
    "        if type(s) == tuple:\n",
    "            np_to_filter = s[1]\n",
    "            if not np_to_filter:\n",
    "                for np in sen['en_sen']:\n",
    "                    if type(np) == tuple:\n",
    "                        if np[0] == s[0]:\n",
    "                            np_to_filter = np[1]\n",
    "            lemmas = filter_nps(' '.join(np_to_filter), \"en\")\n",
    "            en_nps[s[0]] = lemmas\n",
    "            \n",
    "    for s in sen['hu_sen']:\n",
    "        if type(s) == tuple:\n",
    "            np_to_filter = s[1]\n",
    "            if not np_to_filter:\n",
    "                for np in sen['hu_sen']:\n",
    "                    if type(np) == tuple:\n",
    "                        if np[0] == s[0]:\n",
    "                            np_to_filter = np[1]\n",
    "            lemmas = filter_nps(' '.join(np_to_filter), \"hu\")\n",
    "            hu_nps[s[0]] = lemmas\n",
    "    \n",
    "            \n",
    "    scores = [[] for i in range(len(en_nps))]\n",
    "    \n",
    "    for en_np in en_nps:\n",
    "        word_to_embed = defaultdict(list)\n",
    "        for word in en_nps[en_np]:\n",
    "            dic_elements = []\n",
    "            w = word.strip(\"-\").lower()\n",
    "            try:\n",
    "                elements = get_nn(w, src_embeddings, src_id2word, tgt_embeddings, tgt_id2word, K=5)\n",
    "            except KeyError:\n",
    "                elements = [w]\n",
    "\n",
    "            for el in elements:\n",
    "                 #for i in el.split():\n",
    "                doc = nlp_hu(el)\n",
    "                lem = doc[0].lemma_\n",
    "                dic_elements.append(el)\n",
    "                dic_elements.append(lem)\n",
    "\n",
    "\n",
    "            add_morphs_en = []\n",
    "            for el in dic_elements:\n",
    "                if count_vowels(el) > 3:\n",
    "                    m = return_morph(el)\n",
    "                    add_morphs_en += m\n",
    "            for addit in add_morphs_en:\n",
    "                if addit not in dic_elements:\n",
    "                    dic_elements.append(addit)\n",
    "            word_to_embed[word] += dic_elements\n",
    "\n",
    "        for hu_np in hu_nps:\n",
    "            l = []\n",
    "            hu_lower = [s.lower() for s in hu_nps[hu_np]]\n",
    "            add_morphs = []\n",
    "            \n",
    "            for low in hu_lower:\n",
    "                if count_vowels(low) >= 3:\n",
    "                    m = return_morph(low)\n",
    "                    add_morphs += m\n",
    "                    \n",
    "            for addit in add_morphs:\n",
    "                if addit not in hu_lower:\n",
    "                    hu_lower.append(addit)\n",
    "                                                \n",
    "            for word in word_to_embed:                       \n",
    "                inter = []\n",
    "                for en_word in word_to_embed[word]:\n",
    "                    for hu_word in hu_lower:\n",
    "                        dis = distance(en_word, hu_word)\n",
    "                        \n",
    "                        if(len(en_word) > 5 or len(hu_word) > 5):\n",
    "                            if dis < 3:\n",
    "                                inter.append(True)\n",
    "                        else:\n",
    "                            if dis < 1:\n",
    "                                inter.append(True)\n",
    "                if len(inter) > 0:\n",
    "                    l.append(True)\n",
    "                   \n",
    "            listmax = max([hu_lower, en_nps[en_np]], key=len)\n",
    "            if len(l) == 0:\n",
    "                score = 0\n",
    "            else:\n",
    "                score = float(l.count(True)/len(l))\n",
    "            scores[en_np].append(score)\n",
    "            \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process(sen):\n",
    "    scores = compute_min_distance_scores(sen)\n",
    "    if scores is None:\n",
    "        return None\n",
    "    aligns = []\n",
    "    for i in range(len(scores)):\n",
    "        for j,k in enumerate(scores[i]):\n",
    "            if float(k) > 0.5:\n",
    "                aligns.append((str(i), str(j)))\n",
    "    return aligns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0', '0'), ('3', '3'), ('4', '2'), ('5', '5')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process(sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "guesses = []\n",
    "senaligns = {}\n",
    "for i,sentence in enumerate(sentences):\n",
    "    print(sentence['id'])\n",
    "    gold = sentence['aligns']\n",
    "    gold_filtered = []\n",
    "    for goldalign in gold:\n",
    "        en = re.findall('\\d+', goldalign[0] )\n",
    "        hu = re.findall('\\d+', goldalign[1] )\n",
    "        gold_filtered.append((str(en[0]), str(hu[0])))\n",
    "    al = process(sentence)\n",
    "    senaligns[sentence['id']] = al\n",
    "    if al is not None:        \n",
    "        for i in al:\n",
    "            if i in gold_filtered:\n",
    "                guesses.append(True)\n",
    "            else:\n",
    "                guesses.append(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7603278688524591\n",
      "18848\n",
      "12200\n",
      "0.4921477079796265\n",
      "0.5975264107188868\n"
     ]
    }
   ],
   "source": [
    "score = float(guesses.count(True)/len(guesses))\n",
    "np_len = 0\n",
    "for sen in sentences:\n",
    "    np_len += len(sen['aligns'])\n",
    "print(score)\n",
    "print(np_len)\n",
    "print(len(guesses))\n",
    "recall = (score * len(guesses)) / np_len\n",
    "f1_score = (2*recall*score)/(recall+score)\n",
    "print(recall)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
