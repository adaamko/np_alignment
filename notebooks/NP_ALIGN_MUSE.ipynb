{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import matplotlib\n",
    "import re\n",
    "import pydot\n",
    "from graphviz import Digraph\n",
    "from graphviz import Source\n",
    "from baseline_utils import process_baseline\n",
    "from nltk.corpus import stopwords\n",
    "import pprint\n",
    "from Levenshtein import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6567"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = process_baseline(\"/home/adaamko/data/1984.sen-aligned.np-aligned.gold\")\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import hu_core_ud_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import emmorphpy.emmorphpy as emmorph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp_hu = hu_core_ud_lg.load()\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import json\n",
    "import request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_vec(emb_path, nmax=50000):\n",
    "    vectors = []\n",
    "    word2id = {}\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            vectors.append(vect)\n",
    "            word2id[word] = len(word2id)\n",
    "            if len(word2id) == nmax:\n",
    "                break\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "src_path = '/home/adaamko/data/DMR/wiki.multi.en.vec'\n",
    "tgt_path = '/home/adaamko/data/DMR/wiki.multi.hu.vec'\n",
    "nmax = 250000  # maximum number of word embeddings to load\n",
    "\n",
    "src_embeddings, src_id2word, src_word2id = load_vec(src_path, nmax)\n",
    "tgt_embeddings, tgt_id2word, tgt_word2id = load_vec(tgt_path, nmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_nn(word, src_emb, src_id2word, tgt_emb, tgt_id2word, K=5):\n",
    "    word2id = {v: k for k, v in src_id2word.items()}\n",
    "    word_emb = src_emb[word2id[word]]\n",
    "    scores = (tgt_emb / np.linalg.norm(tgt_emb, 2, 1)[:, None]).dot(word_emb / np.linalg.norm(word_emb))\n",
    "    k_best = scores.argsort()[-K:][::-1]\n",
    "    for i, idx in enumerate(k_best):\n",
    "        print('%.4f - %s' % (scores[idx], tgt_id2word[idx]))\n",
    "    words = []\n",
    "    for i, idx in enumerate(k_best):\n",
    "        words.append(tgt_id2word[idx])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_word2id = {v: k for k, v in src_id2word.items()}\n",
    "tgt_word2id = {v: k for k, v in tgt_id2word.items()}\n",
    "\n",
    "def get_distance(src_word, tgt_word, src_emb, tgt_emb):\n",
    "    src_word_emb = src_emb[src_word2id[src_word]]\n",
    "    tgt_word_emb = tgt_emb[tgt_word2id[tgt_word]]\n",
    "    score = (tgt_word_emb / np.linalg.norm(tgt_word_emb)).dot(src_word_emb / np.linalg.norm(src_word_emb))\n",
    "    return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7416 - tizenhárom\n",
      "0.7202 - nyolc\n",
      "0.7166 - tizenegy\n",
      "0.7019 - kilenc\n",
      "0.7015 - nyolcvanhárom\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tizenhárom', 'nyolc', 'tizenegy', 'kilenc', 'nyolcvanhárom']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_word = 'thirteen'\n",
    "get_nn(src_word, src_embeddings, src_id2word, tgt_embeddings, tgt_id2word, K=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7415513806514814"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_word = \"thirteen\"\n",
    "tgt_word = \"tizenhárom\"\n",
    "\n",
    "get_distance(src_word, tgt_word, src_embeddings, tgt_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_nps(nps, language):\n",
    "    lemmas = []\n",
    "    if language == \"hu\":\n",
    "        #words = [word for word in nps if word.isupper() == False]\n",
    "        #words = [word.lower() for word in words if word.lower() not in stopwords.words('hungarian')]\n",
    "        doc = nlp_hu(nps.lower())\n",
    "        \n",
    "    elif language == \"en\":\n",
    "        doc = nlp_en(nps.lower())\n",
    "        \n",
    "    words = []\n",
    "        \n",
    "    for ent in doc:\n",
    "        if not ent.is_stop:\n",
    "            if ent.lemma_ == \"-PRON-\":\n",
    "                words.append(ent.lower_)\n",
    "            else:\n",
    "                words.append(ent.lemma_.lower())\n",
    "                words.append(ent.lower_)\n",
    "\n",
    "    if not words:\n",
    "        for ent in doc:\n",
    "            words.append(ent.lower_)\n",
    "                \n",
    "    lemmas += words\n",
    "        \n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def return_morph(word):\n",
    "    ws = json.loads(requests.get('https://emmorph.herokuapp.com/analyze/' + word).text)[word]\n",
    "    ret_list = []\n",
    "    s = [i['morphana'] for i in ws]\n",
    "    for i in s:\n",
    "        i = i.split(\"=\")\n",
    "        morpheme = i[1]\n",
    "        morpheme = morpheme.split(\"+\")\n",
    "        for m in morpheme:\n",
    "            mor = m.split(\"[\")[0].strip()\n",
    "            if len(mor) > 2:\n",
    "                ret_list.append(m.split(\"[\")[0].strip())\n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_vowels(word):\n",
    "    c = {v:word.count(v) for v in 'aeuioáéúüűíóöő'}\n",
    "    count = sum(c.values())\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_min_distance_scores(sen):\n",
    "    en_nps = {}\n",
    "    hu_nps = {}\n",
    "    for s in sen['en_sen']:\n",
    "        if type(s) == tuple:\n",
    "            np_to_filter = s[1]\n",
    "            if not np_to_filter:\n",
    "                for np in sen['en_sen']:\n",
    "                    if type(np) == tuple:\n",
    "                        if np[0] == s[0]:\n",
    "                            np_to_filter = np[1]\n",
    "            lemmas = filter_nps(' '.join(np_to_filter), \"en\")\n",
    "            en_nps[s[0]] = lemmas\n",
    "            \n",
    "    for s in sen['hu_sen']:\n",
    "        if type(s) == tuple:\n",
    "            np_to_filter = s[1]\n",
    "            if not np_to_filter:\n",
    "                for np in sen['hu_sen']:\n",
    "                    if type(np) == tuple:\n",
    "                        if np[0] == s[0]:\n",
    "                            np_to_filter = np[1]\n",
    "            lemmas = filter_nps(' '.join(np_to_filter), \"hu\")\n",
    "            hu_nps[s[0]] = lemmas\n",
    "    \n",
    "    scores = [[] for i in range(len(en_nps))]\n",
    "    print(en_nps)\n",
    "    print(hu_nps)\n",
    "    for en_np in en_nps:\n",
    "        for hu_np in hu_nps:\n",
    "            hu_lower = [s.lower() for s in hu_nps[hu_np]]\n",
    "            add_morphs = []\n",
    "            \n",
    "            for low in hu_lower:\n",
    "                if count_vowels(low) >= 3:\n",
    "                    m = return_morph(low)\n",
    "                    add_morphs += m\n",
    "                    \n",
    "            for addit in add_morphs:\n",
    "                if addit not in hu_lower:\n",
    "                    hu_lower.append(addit)\n",
    "                    \n",
    "            max_score = 0\n",
    "            for word in en_nps[en_np]:\n",
    "                w = word.strip(\"-\").lower()\n",
    "                for hu_word in hu_lower:\n",
    "                    try:\n",
    "                        distance = get_distance(w, hu_word, src_embeddings, tgt_embeddings)\n",
    "                    except KeyError:\n",
    "                        distance = 0\n",
    "                    if distance > max_score:\n",
    "                        max_score = distance\n",
    "            scores[en_np].append(max_score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_KNN_scores(sen):\n",
    "    en_nps = {}\n",
    "    hu_nps = {}\n",
    "    for s in sen['en_sen']:\n",
    "        if type(s) == tuple:\n",
    "            np_to_filter = s[1]\n",
    "            if not np_to_filter:\n",
    "                for np in sen['en_sen']:\n",
    "                    if type(np) == tuple:\n",
    "                        if np[0] == s[0]:\n",
    "                            np_to_filter = np[1]\n",
    "            lemmas = filter_nps(' '.join(np_to_filter), \"en\")\n",
    "            en_nps[s[0]] = lemmas\n",
    "            \n",
    "    for s in sen['hu_sen']:\n",
    "        if type(s) == tuple:\n",
    "            np_to_filter = s[1]\n",
    "            if not np_to_filter:\n",
    "                for np in sen['hu_sen']:\n",
    "                    if type(np) == tuple:\n",
    "                        if np[0] == s[0]:\n",
    "                            np_to_filter = np[1]\n",
    "            lemmas = filter_nps(' '.join(np_to_filter), \"hu\")\n",
    "            hu_nps[s[0]] = lemmas\n",
    "    \n",
    "            \n",
    "    scores = [[] for i in range(len(en_nps))]\n",
    "    \n",
    "    for en_np in en_nps:\n",
    "        word_to_embed = defaultdict(list)\n",
    "        for word in en_nps[en_np]:\n",
    "            dic_elements = []\n",
    "            w = word.strip(\"-\").lower()\n",
    "            try:\n",
    "                elements = get_nn(w, src_embeddings, src_id2word, tgt_embeddings, tgt_id2word, K=5)\n",
    "            except KeyError:\n",
    "                elements = [w]\n",
    "\n",
    "            for el in elements:\n",
    "                 #for i in el.split():\n",
    "                doc = nlp_hu(el)\n",
    "                lem = doc[0].lemma_\n",
    "                dic_elements.append(el)\n",
    "                dic_elements.append(lem)\n",
    "\n",
    "\n",
    "            add_morphs_en = []\n",
    "            for el in dic_elements:\n",
    "                if count_vowels(el) > 3:\n",
    "                    m = return_morph(el)\n",
    "                    add_morphs_en += m\n",
    "            for addit in add_morphs_en:\n",
    "                if addit not in dic_elements:\n",
    "                    dic_elements.append(addit)\n",
    "            word_to_embed[word] += dic_elements\n",
    "\n",
    "        for hu_np in hu_nps:\n",
    "            l = []\n",
    "            hu_lower = [s.lower() for s in hu_nps[hu_np]]\n",
    "            add_morphs = []\n",
    "            \n",
    "            for low in hu_lower:\n",
    "                if count_vowels(low) >= 3:\n",
    "                    m = return_morph(low)\n",
    "                    add_morphs += m\n",
    "                    \n",
    "            for addit in add_morphs:\n",
    "                if addit not in hu_lower:\n",
    "                    hu_lower.append(addit)\n",
    "                                                \n",
    "            for word in word_to_embed:                       \n",
    "                inter = []\n",
    "                for en_word in word_to_embed[word]:\n",
    "                    for hu_word in hu_lower:\n",
    "                        dis = distance(en_word, hu_word)\n",
    "                        \n",
    "                        if(len(en_word) > 5 or len(hu_word) > 5):\n",
    "                            if dis < 3:\n",
    "                                inter.append(True)\n",
    "                        else:\n",
    "                            if dis < 1:\n",
    "                                inter.append(True)\n",
    "                if len(inter) > 0:\n",
    "                    l.append(True)\n",
    "                   \n",
    "            listmax = max([hu_lower, en_nps[en_np]], key=len)\n",
    "            if len(l) == 0:\n",
    "                score = 0\n",
    "            else:\n",
    "                score = float(l.count(True)/len(l))\n",
    "            scores[en_np].append(score)\n",
    "            \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process(sen):\n",
    "    scores = compute_min_distance_scores(sen)\n",
    "    if scores is None:\n",
    "        return None\n",
    "    aligns = []\n",
    "    for i in range(len(scores)):\n",
    "        for j,k in enumerate(scores[i]):\n",
    "            if float(k) > 0.5:\n",
    "                aligns.append((str(i), str(j)))\n",
    "    return aligns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ['it'], 1: ['bright', 'bright', 'cold', 'cold', 'day', 'day', 'april', 'april'], 2: ['clock', 'clocks'], 3: ['thirteen', 'thirteen']}\n",
      "{0: ['derül', 'derült', ',', ',', 'hideg', 'hideg', 'áprilisi', 'áprilisi', 'nap', 'nap'], 1: ['óra', 'órák'], 2: ['tizenhármat', 'tizenhármat']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('1', '0')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "guesses = []\n",
    "senaligns = {}\n",
    "for i,sentence in enumerate(sentences):\n",
    "    print(sentence['id'])\n",
    "    if i > 100:\n",
    "        break\n",
    "    gold = sentence['aligns']\n",
    "    gold_filtered = []\n",
    "    for goldalign in gold:\n",
    "        en = re.findall('\\d+', goldalign[0] )\n",
    "        hu = re.findall('\\d+', goldalign[1] )\n",
    "        gold_filtered.append((str(en[0]), str(hu[0])))\n",
    "    al = process(sentence)\n",
    "    senaligns[sentence['id']] = al\n",
    "    if al is not None:        \n",
    "        for i in al:\n",
    "            if i in gold_filtered:\n",
    "                guesses.append(True)\n",
    "            else:\n",
    "                guesses.append(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6572164948453608\n",
      "390\n",
      "388\n",
      "0.6538461538461539\n",
      "0.6555269922879178\n"
     ]
    }
   ],
   "source": [
    "score = float(guesses.count(True)/len(guesses))\n",
    "np_len = 0\n",
    "for sen in sentences[:101]:\n",
    "    np_len += len(sen['aligns'])\n",
    "print(score)\n",
    "print(np_len)\n",
    "print(len(guesses))\n",
    "recall = (score * len(guesses)) / np_len\n",
    "f1_score = (2*recall*score)/(recall+score)\n",
    "print(recall)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
