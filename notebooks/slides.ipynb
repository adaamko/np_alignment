{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from baseline_utils import process_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6567"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = process_baseline(\"/home/adaamko/data/1984.sen-aligned.np-aligned.gold\")\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Better together: modern methods plus traditional thinking in NP alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview\n",
    "+ We study a typical intermediary task to Machine Translation, the alignment of NPs in the bitext.\n",
    "+ We present simple, dictionary- and word vector-based baselines and a BERT-based system.\n",
    "+ We combine BERT based methods with simple baselines such as stopword removal, lemmatization, and dictionaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data\n",
    "\n",
    "+ The dataset is a manually translated and word-aligned corpus of Orwell’s 1984 \n",
    "+ Was created as part of the MULTEX-East project (Erjavec, 2004).\n",
    "+ A phrase-level alignment between English and Hungarian noun phrases (NPs) was presented in (Recski et al., 2010)  \n",
    "* 6567 sentence pairs\n",
    " * 25,561 English\n",
    " * 22,408 Hungarian NPs\n",
    " * Only top-level NPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Data\n",
    "\n",
    "+ In this context we reduce the task of deciding for a pair of NPs whether they should be aligned.\n",
    "+ We extract all NP candidates from the data.\n",
    "+ The dataset contains 121,783 NP pairs\n",
    " + 18,5 per sentence\n",
    " + 18,789 labeled as alignment (2.9 per sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**[ It ]** 0 was **[ a bright cold day in April ]** 1 , and **[ the clocks ]** 2 were striking **[ thirteen ]** 3 .\n",
    "\n",
    "**[ Derült , hideg áprilisi nap ]** 0 volt , **[ az órák ]** 1 éppen **[ tizenhármat ]** 2 ütöttek .\n",
    "\n",
    "1-0\n",
    "\n",
    "2-1\n",
    "\n",
    "3-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**[ It ]** 0 depicted simply **[ an enormous face , more than a metre wide ]** 1 : **[ the face of a man of about forty -five , with a heavy black moustache and ruggedly handsome features ]** 2 .\n",
    "\n",
    "Csak **[ egy hatalmas arc ]** 0 volt **[ látható ]** 1 **[ rajta ]** 2 , **[ méternél is szélesebb arc ]** 3 : **[ egy negyvenöt év körüli , sűrű fekete bajuszos , durva vonású férfi arca ]** 4 .\n",
    "   \n",
    "0-2 \n",
    "\n",
    "1-0 \n",
    "\n",
    "1-3b \n",
    "\n",
    "2-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'en_sen': [(0, ['It']),\n",
       "  'was',\n",
       "  (1, ['a', 'bright', 'cold', 'day', 'in', 'April']),\n",
       "  ',',\n",
       "  'and',\n",
       "  (2, ['the', 'clocks']),\n",
       "  'were',\n",
       "  'striking',\n",
       "  (3, ['thirteen']),\n",
       "  '.'],\n",
       " 'hu_sen': [(0, ['Derült', ',', 'hideg', 'áprilisi', 'nap']),\n",
       "  'volt',\n",
       "  ',',\n",
       "  (1, ['az', 'órák']),\n",
       "  'éppen',\n",
       "  (2, ['tizenhármat']),\n",
       "  'ütöttek',\n",
       "  '.'],\n",
       " 'sentence_hun': None,\n",
       " 'aligns': [('1', '0'), ('2', '1'), ('3', '2')]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Methods\n",
    "\n",
    "+ Our simplest method relies on MUSE embeddings\n",
    "+ We obtain bag-of-words representation of NPs\n",
    " + remove stopwords using NLTK(Bird et al., 2009)\n",
    " + lemmatize using spacy(Honnibal and Montani, 2017) and emmorph(Novak et al., 2016) \n",
    " + we leave NPs unchanged that contains only stopwords\n",
    "+ We align two NPs if the maximum cosine similarity between any two words are above a threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Methods\n",
    "\n",
    "+ Based on the training dataset, we set this threshold to 0,46.\n",
    "+ If all the words in the NP are outside OOV, we add an edge based on Levenshtein distance.\n",
    " +  we find proper nouns such as Oceania and Óceánia\n",
    "\n",
    "![threshold](https://github.com/adaamko/np_alignment/blob/master/docs/threshold.JPG?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## BERT\n",
    "\n",
    "+ We use the multilingual BERT model.\n",
    "+ For each pair of sentence\n",
    " + we obtain BERT word embeddings by concatenating the sentences together\n",
    " + and using it as an input to the pretrained model\n",
    " + we use the weights of its last 4 hidden layers\n",
    " + we only keep the embeddings of the words\n",
    "+ We use the word embeddings and feed it into an LSTM layer and then a linear layer to predict the probability of aligning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "_It  was no use trying the lift . [SEP] A felvonóval nem volt érdemes próbálkozni ._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## BERT\n",
    "\n",
    "+ There are approximately 6 times more negative samples than true edges\n",
    "+ We experimented with:\n",
    " + weighted loss functions\n",
    " + over- and under-sampling\n",
    "+ The best results were achieved by oversampling positive examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dictionary-based alignment\n",
    "\n",
    "+ Our baseline uses English-Hungarian translation pairs from Wikt2dict and Hokoto.\n",
    "+ After performing stopword removal and lemmatization we retrieve translations from the dictionaries.\n",
    "+ If there is a match we add an alignment edge.\n",
    "+ For words that are at least 5 characters long, a Levenshtein distance not greater than 3 is enough for the words to be considered a match.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Results\n",
    "\n",
    "+ We split the data into train and test portions.\n",
    "+ The test dataset contains 24,357 NP pairs, of which 3,758 (15.43%) are connected by a gold alignment edge.\n",
    "\n",
    "<div id=\"table:baselines\">\n",
    "\n",
    "| <span>**Method** </span> | <span>**Precision** </span> | <span>**Recall** </span> | <span>**F-score** </span> |\n",
    "| :----------------------- | --------------------------: | -----------------------: | ------------------------: |\n",
    "| always yes               |                       15.43 |                      100 |                     26.73 |\n",
    "| surface                  |                       22.30 |                    38.27 |                     28.18 |\n",
    "| MUSE                     |                       63.51 |                    66.29 |                     64.87 |\n",
    "| MUSE+surface             |                       63.52 |                    67.96 |                     65.66 |\n",
    "| BERT                     |                       67.06 |   <span>**77.20**</span> |                     71.77 |\n",
    "| Dict                     |                       77.49 |                    72.01 |                     74.65 |\n",
    "| Dict+surface             |      <span>**78.08**</span> |                    76.66 |    <span>**77.36**</span> |\n",
    "\n",
    "                        Maximum precision, recall and F-score of the systems.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Results\n",
    "\n",
    "+ We also experimented with some voting schemes \n",
    "\n",
    "<div id=\"table:hybrid\">\n",
    "\n",
    "| <span>**Method** </span>     | <span>**Precision** </span> | <span>**Recall** </span> | <span>**F-score** </span> |\n",
    "| :--------------------------- | --------------------------: | -----------------------: | ------------------------: |\n",
    "| BERT \\(\\vee\\) Dict+surface   |                       62.61 |                    90.77 |                     74.10 |\n",
    "| BERT \\(\\wedge\\) Dict+surface |                       92.33 |                    63.09 |                     74.96 |\n",
    "| 3-way vote                   |                       82.30 |                    78.79 |                     80.51 |\n",
    "\n",
    "                                   Performance of hybrid systems\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python37364bitbaseconda27c4385cd81d42338dc23456e05b5ed4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
