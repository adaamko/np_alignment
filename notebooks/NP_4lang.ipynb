{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import matplotlib\n",
    "import re\n",
    "import pydot\n",
    "from graphviz import Digraph\n",
    "from graphviz import Source\n",
    "from baseline_4lang import Utils\n",
    "from baseline_utils import process_baseline\n",
    "from nltk.corpus import stopwords\n",
    "import pprint\n",
    "from Levenshtein import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph\n",
    "from collections import defaultdict\n",
    "utils = Utils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary = defaultdict(list)\n",
    "with open(\"/home/adaamko/tools/wikt2dict/dat/wiktionary/Hungarian/dictionary\", \"r+\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip().split(\"\\t\")\n",
    "        if line[0] == \"en\":\n",
    "            dictionary[line[1].lower()].append(line[3].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "dictionary_hok = defaultdict(list)\n",
    "count=0\n",
    "with open(\"/home/adaamko/data/hokoto\", errors=\"replace\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip().split(\"@\")\n",
    "        if count<33:\n",
    "            print(line)\n",
    "        count+=1\n",
    "        dictionary_hok[line[0].lower()].append(line[2].lower())\n",
    "        \n",
    "old_char = [\"a1\", \"e1\", \"u1\", \"i1\", \"o1\", \"A1\", \"E1\", \"U1\", \"I1\", \"O1\", \"o2\", \"u2\", \"O2\", \"U2\", \"o3\", \"u3\", \"O3\", \"U3\", \"_\"]\n",
    "new_char = [\"á\", \"é\", \"ú\", \"í\", \"ó\", \"Á\", \"É\", \"Ú\", \"Í\", \"Ó\", \"ö\", \"ü\", \"Ö\", \"Ü\", \"ő\", \"ű\", \"Ő\", \"Ű\", \" \"]\n",
    "\n",
    "for i in dictionary_hok:\n",
    "    for j in range(len(dictionary_hok[i])):\n",
    "        for k in range(len(old_char)):\n",
    "            dictionary_hok[i][j] = dictionary_hok[i][j].replace(old_char[k], new_char[k])\n",
    "            \n",
    "for word_hok in dictionary_hok:\n",
    "    words = dictionary_hok[word_hok]\n",
    "    dictionary[word_hok] += words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fourlang_en = defaultdict(list)\n",
    "fourlang_hu = defaultdict(list)\n",
    "with open(\"/home/adaamko/projects/4lang/1200.tsv\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip().split('\\t')\n",
    "        en = line[0].lower()\n",
    "        hu = line[1].lower()\n",
    "        for k in range(len(old_char)):\n",
    "            hu = hu.replace(old_char[k], new_char[k])\n",
    "        fourlang_en[en].append(hu)\n",
    "        fourlang_hu[hu].append(en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence = 'polga1rmester'\n",
    "data = {'word':   sentence}\n",
    "data_json = json.dumps(data)\n",
    "payload = {'json_payload': data_json}\n",
    "headers = {'Content-type': 'application/json', 'Accept': 'text/plain'}\n",
    "r = requests.post(\"http://hlt.bme.hu/4lang/defhun\", data=data_json, headers=headers)\n",
    "s_machines = r.json()['word']\n",
    "\n",
    "g_sen_en = json_graph.adjacency.adjacency_graph(s_machines)\n",
    "s_dot = utils.to_dot(g_sen_en)\n",
    "v_sen = Source(s_dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n",
       " -->\n",
       "<!-- Title: finite_state_machine Pages: 1 -->\n",
       "<svg width=\"317pt\" height=\"518pt\"\n",
       " viewBox=\"0.00 0.00 325.65 532.77\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(0.972222 0.972222) rotate(0) translate(4 528.767)\">\n",
       "<title>finite_state_machine</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-528.767 321.645,-528.767 321.645,4 -4,4\"/>\n",
       "<!-- X1950_140698140684816 -->\n",
       "<g id=\"node1\" class=\"node\"><title>X1950_140698140684816</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"34.4469\" cy=\"-172.539\" rx=\"34.394\" ry=\"34.394\"/>\n",
       "<text text-anchor=\"middle\" x=\"34.4469\" y=\"-168.839\" font-family=\"Times,serif\" font-size=\"14.00\">X1950</text>\n",
       "</g>\n",
       "<!-- at_140698140685072 -->\n",
       "<g id=\"node2\" class=\"node\"><title>at_140698140685072</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"101.447\" cy=\"-463.022\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"101.447\" y=\"-459.322\" font-family=\"Times,serif\" font-size=\"14.00\">at</text>\n",
       "</g>\n",
       "<!-- előtt_140698140685584 -->\n",
       "<g id=\"node5\" class=\"node\"><title>előtt_140698140685584</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"34.4469\" cy=\"-304.132\" rx=\"26.7961\" ry=\"26.7961\"/>\n",
       "<text text-anchor=\"middle\" x=\"34.4469\" y=\"-300.432\" font-family=\"Times,serif\" font-size=\"14.00\">előtt</text>\n",
       "</g>\n",
       "<!-- at_140698140685072&#45;&gt;előtt_140698140685584 -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>at_140698140685072&#45;&gt;előtt_140698140685584</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M99.2203,-445.156C96.2505,-425.902 89.9754,-393.779 78.4469,-368.278 72.8876,-355.981 64.8661,-343.558 57.1906,-333.023\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"59.9739,-330.9 51.1628,-325.017 54.3817,-335.111 59.9739,-330.9\"/>\n",
       "<text text-anchor=\"middle\" x=\"87.9469\" y=\"-372.078\" font-family=\"Times,serif\" font-size=\"14.00\">2</text>\n",
       "</g>\n",
       "<!-- külföld_140698140685712 -->\n",
       "<g id=\"node6\" class=\"node\"><title>külföld_140698140685712</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"226.447\" cy=\"-304.132\" rx=\"36.2938\" ry=\"36.2938\"/>\n",
       "<text text-anchor=\"middle\" x=\"226.447\" y=\"-300.432\" font-family=\"Times,serif\" font-size=\"14.00\">külföld</text>\n",
       "</g>\n",
       "<!-- at_140698140685072&#45;&gt;külföld_140698140685712 -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>at_140698140685072&#45;&gt;külföld_140698140685712</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M103.85,-444.956C107.596,-424.311 116.683,-389.67 137.447,-368.278 151.877,-353.412 163.188,-361.74 180.447,-350.278 185.463,-346.947 190.403,-343.021 195.077,-338.898\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"197.722,-341.218 202.658,-331.844 192.953,-336.093 197.722,-341.218\"/>\n",
       "<text text-anchor=\"middle\" x=\"140.947\" y=\"-372.078\" font-family=\"Times,serif\" font-size=\"14.00\">1</text>\n",
       "</g>\n",
       "<!-- at_140698140685456 -->\n",
       "<g id=\"node3\" class=\"node\"><title>at_140698140685456</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"299.447\" cy=\"-463.022\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"299.447\" y=\"-459.322\" font-family=\"Times,serif\" font-size=\"14.00\">at</text>\n",
       "</g>\n",
       "<!-- at_140698140685456&#45;&gt;előtt_140698140685584 -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>at_140698140685456&#45;&gt;előtt_140698140685584</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M296.189,-445.156C292.659,-431.395 285.66,-412.462 272.447,-401.278 271.046,-400.092 146.228,-368.724 144.447,-368.278 111.614,-360.053 98.9778,-368.49 70.4469,-350.278 64.429,-346.437 58.9732,-341.239 54.2326,-335.736\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"56.7498,-333.275 47.8165,-327.579 51.2478,-337.602 56.7498,-333.275\"/>\n",
       "<text text-anchor=\"middle\" x=\"204.947\" y=\"-372.078\" font-family=\"Times,serif\" font-size=\"14.00\">2</text>\n",
       "</g>\n",
       "<!-- és_140698140684944 -->\n",
       "<g id=\"node12\" class=\"node\"><title>és_140698140684944</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"299.447\" cy=\"-304.132\" rx=\"18.399\" ry=\"18.399\"/>\n",
       "<text text-anchor=\"middle\" x=\"299.447\" y=\"-300.432\" font-family=\"Times,serif\" font-size=\"14.00\">és</text>\n",
       "</g>\n",
       "<!-- at_140698140685456&#45;&gt;és_140698140684944 -->\n",
       "<g id=\"edge4\" class=\"edge\"><title>at_140698140685456&#45;&gt;és_140698140684944</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M299.447,-444.934C299.447,-417.962 299.447,-365.263 299.447,-332.833\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"302.947,-332.679 299.447,-322.679 295.947,-332.679 302.947,-332.679\"/>\n",
       "<text text-anchor=\"middle\" x=\"302.947\" y=\"-372.078\" font-family=\"Times,serif\" font-size=\"14.00\">1</text>\n",
       "</g>\n",
       "<!-- at_140698140685968 -->\n",
       "<g id=\"node4\" class=\"node\"><title>at_140698140685968</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"40.4469\" cy=\"-463.022\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"40.4469\" y=\"-459.322\" font-family=\"Times,serif\" font-size=\"14.00\">at</text>\n",
       "</g>\n",
       "<!-- at_140698140685968&#45;&gt;előtt_140698140685584 -->\n",
       "<g id=\"edge5\" class=\"edge\"><title>at_140698140685968&#45;&gt;előtt_140698140685584</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M39.7934,-444.934C38.8452,-420.138 37.0653,-373.599 35.8207,-341.054\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"39.3168,-340.883 35.4371,-331.024 32.3219,-341.151 39.3168,-340.883\"/>\n",
       "<text text-anchor=\"middle\" x=\"40.9469\" y=\"-372.078\" font-family=\"Times,serif\" font-size=\"14.00\">2</text>\n",
       "</g>\n",
       "<!-- tisztviselő_140698140685328 -->\n",
       "<g id=\"node8\" class=\"node\"><title>tisztviselő_140698140685328</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"125.447\" cy=\"-304.132\" rx=\"46.2923\" ry=\"46.2923\"/>\n",
       "<text text-anchor=\"middle\" x=\"125.447\" y=\"-300.432\" font-family=\"Times,serif\" font-size=\"14.00\">tisztviselő</text>\n",
       "</g>\n",
       "<!-- at_140698140685968&#45;&gt;tisztviselő_140698140685328 -->\n",
       "<g id=\"edge6\" class=\"edge\"><title>at_140698140685968&#45;&gt;tisztviselő_140698140685328</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M44.1591,-445.327C48.9067,-425.95 58.252,-393.442 72.4469,-368.278 76.6926,-360.752 81.8835,-353.263 87.3377,-346.213\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"90.2746,-348.147 93.8206,-338.163 84.8225,-343.756 90.2746,-348.147\"/>\n",
       "<text text-anchor=\"middle\" x=\"75.9469\" y=\"-372.078\" font-family=\"Times,serif\" font-size=\"14.00\">1</text>\n",
       "</g>\n",
       "<!-- előtt_140698140685584&#45;&gt;X1950_140698140684816 -->\n",
       "<g id=\"edge7\" class=\"edge\"><title>előtt_140698140685584&#45;&gt;X1950_140698140684816</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M34.4469,-277.218C34.4469,-260.11 34.4469,-237.262 34.4469,-217.369\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"37.947,-217.282 34.4469,-207.282 30.947,-217.282 37.947,-217.282\"/>\n",
       "<text text-anchor=\"middle\" x=\"37.9469\" y=\"-228.786\" font-family=\"Times,serif\" font-size=\"14.00\">0</text>\n",
       "</g>\n",
       "<!-- vezető_140698140685840 -->\n",
       "<g id=\"node10\" class=\"node\"><title>vezető_140698140685840</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"278.447\" cy=\"-172.539\" rx=\"33.5952\" ry=\"33.5952\"/>\n",
       "<text text-anchor=\"middle\" x=\"278.447\" y=\"-168.839\" font-family=\"Times,serif\" font-size=\"14.00\">vezető</text>\n",
       "</g>\n",
       "<!-- külföld_140698140685712&#45;&gt;vezető_140698140685840 -->\n",
       "<g id=\"edge8\" class=\"edge\"><title>külföld_140698140685712&#45;&gt;vezető_140698140685840</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M239.703,-270.095C246.558,-253.011 254.983,-232.014 262.224,-213.969\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"265.632,-214.875 266.108,-204.291 259.135,-212.268 265.632,-214.875\"/>\n",
       "<text text-anchor=\"middle\" x=\"259.947\" y=\"-228.786\" font-family=\"Times,serif\" font-size=\"14.00\">0</text>\n",
       "</g>\n",
       "<!-- város_140698140685200 -->\n",
       "<g id=\"node11\" class=\"node\"><title>város_140698140685200</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"196.447\" cy=\"-172.539\" rx=\"29.795\" ry=\"29.795\"/>\n",
       "<text text-anchor=\"middle\" x=\"196.447\" y=\"-168.839\" font-family=\"Times,serif\" font-size=\"14.00\">város</text>\n",
       "</g>\n",
       "<!-- külföld_140698140685712&#45;&gt;város_140698140685200 -->\n",
       "<g id=\"edge9\" class=\"edge\"><title>külföld_140698140685712&#45;&gt;város_140698140685200</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M215.385,-269.265C212.549,-259.85 209.674,-249.577 207.447,-239.986 205.391,-231.131 203.569,-221.527 202.029,-212.444\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"205.469,-211.79 200.412,-202.48 198.559,-212.911 205.469,-211.79\"/>\n",
       "<text text-anchor=\"middle\" x=\"210.947\" y=\"-228.786\" font-family=\"Times,serif\" font-size=\"14.00\">0</text>\n",
       "</g>\n",
       "<!-- polga1rmester_140698140684624 -->\n",
       "<g id=\"node7\" class=\"node\"><title>polga1rmester_140698140684624</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"201.447\" cy=\"-463.022\" rx=\"61.99\" ry=\"61.99\"/>\n",
       "<text text-anchor=\"middle\" x=\"201.447\" y=\"-459.322\" font-family=\"Times,serif\" font-size=\"14.00\">polga1rmester</text>\n",
       "</g>\n",
       "<!-- polga1rmester_140698140684624&#45;&gt;külföld_140698140685712 -->\n",
       "<g id=\"edge10\" class=\"edge\"><title>polga1rmester_140698140684624&#45;&gt;külföld_140698140685712</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M211.041,-401.813C213.752,-384.801 216.656,-366.576 219.204,-350.588\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"222.704,-350.865 220.821,-340.439 215.791,-349.764 222.704,-350.865\"/>\n",
       "<text text-anchor=\"middle\" x=\"219.947\" y=\"-372.078\" font-family=\"Times,serif\" font-size=\"14.00\">0</text>\n",
       "</g>\n",
       "<!-- tisztviselő_140698140685328&#45;&gt;vezető_140698140685840 -->\n",
       "<g id=\"edge11\" class=\"edge\"><title>tisztviselő_140698140685328&#45;&gt;vezető_140698140685840</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M151.935,-266.048C163.465,-251.831 177.896,-236.32 193.447,-224.986 209.859,-213.025 217.955,-217.305 235.447,-206.986 238.471,-205.202 241.522,-203.232 244.53,-201.168\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"246.77,-203.867 252.827,-195.175 242.671,-198.192 246.77,-203.867\"/>\n",
       "<text text-anchor=\"middle\" x=\"197.947\" y=\"-228.786\" font-family=\"Times,serif\" font-size=\"14.00\">0</text>\n",
       "</g>\n",
       "<!-- tisztviselő_140698140685328&#45;&gt;város_140698140685200 -->\n",
       "<g id=\"edge12\" class=\"edge\"><title>tisztviselő_140698140685328&#45;&gt;város_140698140685200</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M127.667,-257.99C129.616,-246.703 132.924,-234.976 138.447,-224.986 144.603,-213.851 154.121,-203.924 163.679,-195.832\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"166.009,-198.451 171.644,-189.479 161.644,-192.978 166.009,-198.451\"/>\n",
       "<text text-anchor=\"middle\" x=\"141.947\" y=\"-228.786\" font-family=\"Times,serif\" font-size=\"14.00\">0</text>\n",
       "</g>\n",
       "<!-- valamely_140698140686096 -->\n",
       "<g id=\"node9\" class=\"node\"><title>valamely_140698140686096</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"196.447\" cy=\"-43.5461\" rx=\"43.5923\" ry=\"43.5923\"/>\n",
       "<text text-anchor=\"middle\" x=\"196.447\" y=\"-39.8461\" font-family=\"Times,serif\" font-size=\"14.00\">valamely</text>\n",
       "</g>\n",
       "<!-- város_140698140685200&#45;&gt;valamely_140698140686096 -->\n",
       "<g id=\"edge13\" class=\"edge\"><title>város_140698140685200&#45;&gt;valamely_140698140686096</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M196.447,-142.404C196.447,-129.196 196.447,-113.149 196.447,-97.9021\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"199.947,-97.5194 196.447,-87.5194 192.947,-97.5195 199.947,-97.5194\"/>\n",
       "<text text-anchor=\"middle\" x=\"199.947\" y=\"-108.892\" font-family=\"Times,serif\" font-size=\"14.00\">0</text>\n",
       "</g>\n",
       "<!-- és_140698140684944&#45;&gt;vezető_140698140685840 -->\n",
       "<g id=\"edge14\" class=\"edge\"><title>és_140698140684944&#45;&gt;vezető_140698140685840</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M296.658,-285.923C293.778,-268.151 289.174,-239.734 285.315,-215.923\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"288.769,-215.356 283.714,-206.044 281.859,-216.475 288.769,-215.356\"/>\n",
       "<text text-anchor=\"middle\" x=\"291.947\" y=\"-228.786\" font-family=\"Times,serif\" font-size=\"14.00\">0</text>\n",
       "</g>\n",
       "<!-- és_140698140684944&#45;&gt;város_140698140685200 -->\n",
       "<g id=\"edge15\" class=\"edge\"><title>és_140698140684944&#45;&gt;város_140698140685200</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M291.982,-287.204C287.074,-277.912 279.987,-266.402 271.447,-257.986 260.371,-247.072 252.739,-250.675 241.447,-239.986 231.627,-230.69 222.785,-218.893 215.591,-207.826\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"218.451,-205.799 210.187,-199.169 212.513,-209.505 218.451,-205.799\"/>\n",
       "<text text-anchor=\"middle\" x=\"244.947\" y=\"-228.786\" font-family=\"Times,serif\" font-size=\"14.00\">0</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x7f66e40b6be0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence = 'this'\n",
    "data = {'word':   sentence}\n",
    "data_json = json.dumps(data)\n",
    "payload = {'json_payload': data_json}\n",
    "headers = {'Content-type': 'application/json', 'Accept': 'text/plain'}\n",
    "r = requests.post(\"http://hlt.bme.hu/4lang/definition\", data=data_json, headers=headers)\n",
    "s_machines = r.json()['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_sen_hu = json_graph.adjacency.adjacency_graph(s_machines)\n",
    "s_dot = utils.to_dot(g_sen_hu)\n",
    "v_sen = Source(s_dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n",
       " -->\n",
       "<!-- Title: finite_state_machine Pages: 1 -->\n",
       "<svg width=\"249pt\" height=\"302pt\"\n",
       " viewBox=\"0.00 0.00 256.14 310.18\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(0.972222 0.972222) rotate(0) translate(4 306.182)\">\n",
       "<title>finite_state_machine</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-306.182 252.145,-306.182 252.145,4 -4,4\"/>\n",
       "<!-- ABOUT_140257900176720 -->\n",
       "<g id=\"node1\" class=\"node\"><title>ABOUT_140257900176720</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"131.147\" cy=\"-261.236\" rx=\"40.8928\" ry=\"40.8928\"/>\n",
       "<text text-anchor=\"middle\" x=\"131.147\" y=\"-257.536\" font-family=\"Times,serif\" font-size=\"14.00\">ABOUT</text>\n",
       "</g>\n",
       "<!-- before_140257900176592 -->\n",
       "<g id=\"node2\" class=\"node\"><title>before_140257900176592</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"33.1471\" cy=\"-136.142\" rx=\"33.2948\" ry=\"33.2948\"/>\n",
       "<text text-anchor=\"middle\" x=\"33.1471\" y=\"-132.442\" font-family=\"Times,serif\" font-size=\"14.00\">before</text>\n",
       "</g>\n",
       "<!-- ABOUT_140257900176720&#45;&gt;before_140257900176592 -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>ABOUT_140257900176720&#45;&gt;before_140257900176592</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M105.91,-228.536C91.7457,-210.746 74.0867,-188.565 59.7124,-170.51\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.3303,-168.179 53.3636,-162.536 56.854,-172.539 62.3303,-168.179\"/>\n",
       "<text text-anchor=\"middle\" x=\"85.6471\" y=\"-191.09\" font-family=\"Times,serif\" font-size=\"14.00\">0</text>\n",
       "</g>\n",
       "<!-- speak_140257900176848 -->\n",
       "<g id=\"node5\" class=\"node\"><title>speak_140257900176848</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"115.147\" cy=\"-136.142\" rx=\"30.5947\" ry=\"30.5947\"/>\n",
       "<text text-anchor=\"middle\" x=\"115.147\" y=\"-132.442\" font-family=\"Times,serif\" font-size=\"14.00\">speak</text>\n",
       "</g>\n",
       "<!-- ABOUT_140257900176720&#45;&gt;speak_140257900176848 -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>ABOUT_140257900176720&#45;&gt;speak_140257900176848</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M122.251,-221.091C121.056,-214.828 119.963,-208.399 119.147,-202.29 118.049,-194.065 117.251,-185.215 116.673,-176.771\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"120.164,-176.513 116.07,-166.741 113.176,-176.933 120.164,-176.513\"/>\n",
       "<text text-anchor=\"middle\" x=\"122.647\" y=\"-191.09\" font-family=\"Times,serif\" font-size=\"14.00\">1</text>\n",
       "</g>\n",
       "<!-- this_140257900176272 -->\n",
       "<g id=\"node6\" class=\"node\"><title>this_140257900176272</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"187.147\" cy=\"-136.142\" rx=\"23.2963\" ry=\"23.2963\"/>\n",
       "<text text-anchor=\"middle\" x=\"187.147\" y=\"-132.442\" font-family=\"Times,serif\" font-size=\"14.00\">this</text>\n",
       "</g>\n",
       "<!-- ABOUT_140257900176720&#45;&gt;this_140257900176272 -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>ABOUT_140257900176720&#45;&gt;this_140257900176272</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M141.847,-221.673C145.512,-210.365 150.005,-198.131 155.147,-187.29 158.819,-179.548 163.473,-171.557 168.085,-164.291\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"171.263,-165.829 173.829,-155.55 165.413,-161.985 171.263,-165.829\"/>\n",
       "<text text-anchor=\"middle\" x=\"158.647\" y=\"-191.09\" font-family=\"Times,serif\" font-size=\"14.00\">2</text>\n",
       "</g>\n",
       "<!-- near_140257900176976 -->\n",
       "<g id=\"node3\" class=\"node\"><title>near_140257900176976</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"152.147\" cy=\"-25.9977\" rx=\"25.9954\" ry=\"25.9954\"/>\n",
       "<text text-anchor=\"middle\" x=\"152.147\" y=\"-22.2977\" font-family=\"Times,serif\" font-size=\"14.00\">near</text>\n",
       "</g>\n",
       "<!-- now_140257900176464 -->\n",
       "<g id=\"node4\" class=\"node\"><title>now_140257900176464</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"222.147\" cy=\"-25.9977\" rx=\"25.9954\" ry=\"25.9954\"/>\n",
       "<text text-anchor=\"middle\" x=\"222.147\" y=\"-22.2977\" font-family=\"Times,serif\" font-size=\"14.00\">now</text>\n",
       "</g>\n",
       "<!-- this_140257900176272&#45;&gt;ABOUT_140257900176720 -->\n",
       "<g id=\"edge4\" class=\"edge\"><title>this_140257900176272&#45;&gt;ABOUT_140257900176720</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M179.699,-158.544C175.04,-171.391 168.695,-187.967 162.147,-202.29 160.119,-206.725 157.904,-211.296 155.62,-215.832\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"152.345,-214.548 150.876,-225.041 158.567,-217.754 152.345,-214.548\"/>\n",
       "<text text-anchor=\"middle\" x=\"172.647\" y=\"-191.09\" font-family=\"Times,serif\" font-size=\"14.00\">0</text>\n",
       "</g>\n",
       "<!-- this_140257900176272&#45;&gt;near_140257900176976 -->\n",
       "<g id=\"edge5\" class=\"edge\"><title>this_140257900176272&#45;&gt;near_140257900176976</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M180.232,-113.777C175.336,-98.6482 168.669,-78.0486 163.06,-60.7161\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"166.31,-59.3909 159.9,-50.9545 159.65,-61.5464 166.31,-59.3909\"/>\n",
       "<text text-anchor=\"middle\" x=\"174.647\" y=\"-73.7954\" font-family=\"Times,serif\" font-size=\"14.00\">0</text>\n",
       "</g>\n",
       "<!-- this_140257900176272&#45;&gt;now_140257900176464 -->\n",
       "<g id=\"edge6\" class=\"edge\"><title>this_140257900176272&#45;&gt;now_140257900176464</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M194.062,-113.777C198.958,-98.6482 205.625,-78.0486 211.234,-60.7161\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"214.644,-61.5464 214.394,-50.9545 207.985,-59.3909 214.644,-61.5464\"/>\n",
       "<text text-anchor=\"middle\" x=\"210.647\" y=\"-73.7954\" font-family=\"Times,serif\" font-size=\"14.00\">0</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x7f15298c4e80>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "en_nodes = utils.get_edges(g_sen_hu)\n",
    "en_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hu_nodes = utils.get_nodes(g_sen_hu)\n",
    "hu_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6567"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = process_baseline(\"/home/adaamko/data/1984.sen-aligned.np-aligned.gold\")\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import spacy\n",
    "import emmorphpy.emmorphpy as emmorph\n",
    "import itertools\n",
    "from itertools import permutations, repeat\n",
    "\n",
    "nlp_en = spacy.load('en')\n",
    "nlp_hu = emmorph.EmMorphPy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k,sentence in enumerate(sentences):\n",
    "    print(k)\n",
    "    sentence['hu_sen_4lang'] = []\n",
    "    sentence['en_sen_4lang'] = []\n",
    "    for np in sentence['en_sen']:\n",
    "        if type(np) == tuple:\n",
    "            sen = ' '.join(np[1])\n",
    "            data = {'sentence':   sen}\n",
    "            data_json = json.dumps(data)\n",
    "            payload = {'json_payload': data_json}\n",
    "            headers = {'Content-type': 'application/json', 'Accept': 'text/plain'}\n",
    "            r = requests.post(\"http://hlt.bme.hu/4lang/senexp\", data=data_json, headers=headers)\n",
    "            s_machines = r.json()['sentence']\n",
    "\n",
    "            g_sen_en = json_graph.adjacency.adjacency_graph(s_machines)\n",
    "            en_nodes = utils.get_nodes(g_sen_en)\n",
    "            sentence[\"en_sen_4lang\"].append((np[0], en_nodes))\n",
    "    for np in sentence['hu_sen']:\n",
    "        if type(np) == tuple:\n",
    "            sen = ' '.join(np[1])\n",
    "            data = {'sentence':   sen}\n",
    "            data_json = json.dumps(data)\n",
    "            payload = {'json_payload': data_json}\n",
    "            headers = {'Content-type': 'application/json', 'Accept': 'text/plain'}\n",
    "            r = requests.post(\"http://hlt.bme.hu/4lang/senexphun\", data=data_json, headers=headers)\n",
    "            s_machines = r.json()['sentence']\n",
    "            \n",
    "            g_sen_hu = json_graph.adjacency.adjacency_graph(s_machines)\n",
    "            en_nodes = utils.get_nodes(g_sen_hu)\n",
    "            sentence[\"hu_sen_4lang\"].append((np[0], en_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('sentences.json', 'w+', encoding='utf-8') as file:\n",
    "     file.write(json.dumps(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_4lang(nps, language):\n",
    "    lemmas = []\n",
    "    if language == \"hu\":\n",
    "        words = [word for word in nps if word.isupper() == False]\n",
    "        words = [word.lower() for word in words if word.lower() not in stopwords.words('hungarian')]\n",
    "        words = [word for word in words if word not in stopwords.words('hungarian')]\n",
    "        words = [word.lower() for word in words if word.lower() not in [\"in\", \"on\", \"root\", \"at\", \"to\", \"has\"]]\n",
    "        if not words:\n",
    "            words = nps\n",
    "        for np in words:\n",
    "            try:\n",
    "                if len(nlp_hu.stem(np)) > 0:\n",
    "                    lemma = nlp_hu.stem(np)[0][0]\n",
    "                    lemmas.append(lemma)\n",
    "                    lemmas.append(np)\n",
    "                else:\n",
    "                    lemmas.append(np)\n",
    "            except (IndexError, NameError) as e:\n",
    "                print(e)\n",
    "                lemmas.append(np)\n",
    "        \n",
    "    if language == \"en\":\n",
    "        words = [word.lower() for word in nps if word.lower() not in stopwords.words('english')]\n",
    "        words = [word for word in words if word not in stopwords.words('english')]\n",
    "        words = [word for word in words if word.startswith(\"x\") == False or word.isupper() == False]\n",
    "        if not words:\n",
    "            words = nps\n",
    "        for np in words:\n",
    "            if len(nlp_en(np)) > 0:\n",
    "                lemma = nlp_en(np)[0].lemma_\n",
    "            else:\n",
    "                lemma = np\n",
    "            if lemma == \"-PRON-\":\n",
    "                lemmas.append(np.lower())\n",
    "            else:\n",
    "                lemmas.append(lemma.lower())\n",
    "                lemmas.append(lemma)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filter_nps(nps, language):\n",
    "    lemmas = []\n",
    "    if language == \"hu\":\n",
    "        #words = [word for word in nps if word.isupper() == False]\n",
    "        #words = [word.lower() for word in words if word.lower() not in stopwords.words('hungarian')]\n",
    "        words = [word for word in nps if word not in stopwords.words('hungarian')]\n",
    "        if not words:\n",
    "            words = nps\n",
    "        for np in words:\n",
    "            try:\n",
    "                if len(nlp_hu.stem(np)) > 0:\n",
    "                    lemma = nlp_hu.stem(np)[0][0]\n",
    "                    lemmas.append(lemma)\n",
    "                    lemmas.append(np)\n",
    "                else:\n",
    "                    lemmas.append(np)\n",
    "            except (IndexError, NameError) as e:\n",
    "                print(e)\n",
    "                lemmas.append(np)\n",
    "        \n",
    "    if language == \"en\":\n",
    "        #words = [word.lower() for word in nps if word.lower() not in stopwords.words('english')]\n",
    "        words = [word for word in nps if word not in stopwords.words('english')]\n",
    "        if not words:\n",
    "            words = nps\n",
    "        for np in words:\n",
    "            if len(nlp_en(np)) > 0:\n",
    "                lemma = nlp_en(np)[0].lemma_\n",
    "            else:\n",
    "                lemma = np\n",
    "            if lemma == \"-PRON-\":\n",
    "                lemmas.append(np.lower())\n",
    "            else:\n",
    "                #lemmas.append(lemma.lower())\n",
    "                lemmas.append(lemma)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process(sen):\n",
    "    scores = compute_scores(sen, dictionary)\n",
    "    if scores is None:\n",
    "        return None\n",
    "    aligns = []\n",
    "    for i in range(len(scores)):\n",
    "        for j,k in enumerate(scores[i]):\n",
    "            if float(k) > 0.001:\n",
    "                aligns.append((str(i), str(j)))\n",
    "    return aligns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_scores(sen, dic):\n",
    "    en_nps = {}\n",
    "    hu_nps = {}\n",
    "    for s in sen['en_sen']:\n",
    "        if type(s) == tuple:\n",
    "            np_to_filter = s[1]\n",
    "            if not np_to_filter:\n",
    "                for np in sen['en_sen']:\n",
    "                    if type(np) == tuple:\n",
    "                        if np[0] == s[0]:\n",
    "                            np_to_filter = np[1]\n",
    "            lemmas = filter_nps(np_to_filter, \"en\")\n",
    "            en_nps[s[0]] = lemmas\n",
    "    for s in sen['hu_sen']:\n",
    "        if type(s) == tuple:\n",
    "            np_to_filter = s[1]\n",
    "            if not np_to_filter:\n",
    "                for np in sen['hu_sen']:\n",
    "                    if type(np) == tuple:\n",
    "                        if np[0] == s[0]:\n",
    "                            np_to_filter = np[1]\n",
    "            lemmas = filter_nps(np_to_filter, \"hu\")\n",
    "            hu_nps[s[0]] = lemmas\n",
    "    \n",
    "            \n",
    "    scores = [[] for i in range(len(en_nps))]\n",
    "    \n",
    "\n",
    "    #dic_elements = defaultdict(list)\n",
    "    #for en_np in en_nps:\n",
    "        #for word in en_nps[en_np]:\n",
    "            #print(word)\n",
    "            #for el in dictionary[word]:\n",
    "                #dic_elements[word].append(el)\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    #pp.pprint(dic_elements)\n",
    "    #pp.pprint(en_nps)\n",
    "    #pp.pprint(hu_nps)\n",
    "    for en_np in en_nps:\n",
    "        for hu_np in hu_nps:\n",
    "            l = []\n",
    "            hu_lower = [s.lower() for s in hu_nps[hu_np]]\n",
    "            add_morphs = []\n",
    "            for low in hu_lower:\n",
    "                if count_vowels(low) > 3:\n",
    "                    ms = morphemes[low]\n",
    "                    add_morphs += ms\n",
    "            for addit in add_morphs:\n",
    "                if addit not in hu_lower:\n",
    "                    hu_lower.append(addit)\n",
    "                            \n",
    "                    \n",
    "            for word in en_nps[en_np]:\n",
    "                dic_elements = []\n",
    "                w = word.strip(\"-\").lower()\n",
    "                if not dictionary[w]:\n",
    "                    dic_elements.append(w)\n",
    "                for el in dictionary[w]:\n",
    "                     #for i in el.split():\n",
    "                    dic_elements.append(el)\n",
    "                inter = []\n",
    "                for en_word in dic_elements:\n",
    "                    for hu_word in hu_lower:\n",
    "                        dis = distance(en_word, hu_word)\n",
    "                        \n",
    "                        if(len(en_word) > 5 or len(hu_word) > 5):\n",
    "                            if dis < 3:\n",
    "                                inter.append(True)\n",
    "                        else:\n",
    "                            if dis < 1:\n",
    "                                inter.append(True)\n",
    "                if len(inter) > 0:\n",
    "                    l.append(True)\n",
    "                else:\n",
    "                    ancestors_word_en = []\n",
    "                    for anc in en_ancestors[word]:\n",
    "                        if anc not in en_ancestor_top:\n",
    "                            ancestors_word_en.append(anc)\n",
    "                    ancestors_word_en.append(word)\n",
    "                    \n",
    "                    ancestors_word_hu = []\n",
    "                    for hu_word in hu_lower:\n",
    "                        ancestors_word_hu.append(hu_word)\n",
    "                        for anc in hu_ancestors[hu_word]:\n",
    "                            if anc not in hu_ancestor_top:\n",
    "                                ancestors_word_hu.append(anc)\n",
    "                    for word_en in ancestors_word_en:\n",
    "                        dic_elements = []\n",
    "                        w = word_en.strip(\"-\").lower() \n",
    "                        if w in stopwords.words(\"english\"):\n",
    "                            pass\n",
    "                        else:\n",
    "                            if not dictionary[w]:\n",
    "                                dic_elements.append(w)\n",
    "                            for el in dictionary[w]:\n",
    "                                if el not in stopwords.words('hungarian'):\n",
    "                                     #for i in el.split():\n",
    "                                    dic_elements.append(el)\n",
    "                            inter = []\n",
    "                        for en_word in dic_elements:\n",
    "                            for hu_word in ancestors_word_hu:\n",
    "                                dis = distance(en_word, hu_word)\n",
    "                                if dis < 1:\n",
    "                                    inter.append(True)\n",
    "                        l.append(len(inter) > 0)\n",
    "            listmax = max([hu_lower, en_nps[en_np]], key=len)\n",
    "            if len(l) == 0:\n",
    "                score = 0\n",
    "            else:\n",
    "                score = float(l.count(True)/len(l))\n",
    "            scores[en_np].append(score)\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def return_morph(word):\n",
    "    s = nlp_hu.analyze(word)\n",
    "    ret_list = []\n",
    "    for i in s:\n",
    "        i = i.split(\"=\")\n",
    "        morpheme = i[1]\n",
    "        morpheme = morpheme.split(\"+\")\n",
    "        for m in morpheme:\n",
    "            mor = m.split(\"[\")[0].strip()\n",
    "            if len(mor) > 2:\n",
    "                ret_list.append(m.split(\"[\")[0].strip())\n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def count_vowels(word):\n",
    "    c = {v:word.count(v) for v in 'aeuioáéúüűíóöő'}\n",
    "    count = sum(c.values())\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Finding the morphemes of the words\n",
    "morphemes = defaultdict(list)\n",
    "for k,sentence in enumerate(sentences):\n",
    "    print(k)\n",
    "    for np in sentence['hu_sen']:\n",
    "        if type(np) == tuple:\n",
    "            hu_lemmas = filter_nps(np[1], \"hu\")\n",
    "            for word in hu_lemmas:\n",
    "                if count_vowels(word) > 3:\n",
    "                    m = return_morph(word)\n",
    "                    if word not in morphemes:\n",
    "                        morphemes[word] += m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "guesses = []\n",
    "senaligns = {}\n",
    "for sentence in sentences:\n",
    "    print(sentence['id'])\n",
    "    gold = sentence['aligns']\n",
    "    gold_filtered = []\n",
    "    for goldalign in gold:\n",
    "        en = re.findall('\\d+', goldalign[0] )\n",
    "        hu = re.findall('\\d+', goldalign[1] )\n",
    "        gold_filtered.append((str(en[0]), str(hu[0])))\n",
    "    al = process(sentence)\n",
    "    senaligns[sentence['id']] = al\n",
    "    if al is not None:        \n",
    "        for i in al:\n",
    "            if i in gold_filtered:\n",
    "                guesses.append(True)\n",
    "            else:\n",
    "                guesses.append(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7405826217980914\n",
      "18848\n",
      "19910\n",
      "0.7823111205432938\n",
      "0.7608751741575933\n"
     ]
    }
   ],
   "source": [
    "score = float(guesses.count(True)/len(guesses))\n",
    "np_len = 0\n",
    "for sen in sentences:\n",
    "    np_len += len(sen['aligns'])\n",
    "print(score)\n",
    "print(np_len)\n",
    "print(len(guesses))\n",
    "recall = (score * len(guesses)) / np_len\n",
    "f1_score = (2*recall*score)/(recall+score)\n",
    "print(recall)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "process(sentences[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hu_ancestors = defaultdict(list)\n",
    "en_ancestors = defaultdict(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Angol ősök kigyujtese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def expand_en_word_all(word):\n",
    "    data = {'word':   word}\n",
    "    data_json = json.dumps(data)\n",
    "    payload = {'json_payload': data_json}\n",
    "    headers = {'Content-type': 'application/json', 'Accept': 'text/plain'}\n",
    "    r = requests.post(\"http://hlt.bme.hu/4lang/definition\", data=data_json, headers=headers)\n",
    "    s_machines = r.json()['word']\n",
    "\n",
    "    g_sen_en = json_graph.adjacency.adjacency_graph(s_machines)\n",
    "    en_edges = utils.get_edges(g_sen_en)\n",
    "    \n",
    "    nodes_to_append = []\n",
    "    for edge in en_edges:\n",
    "        if edge[2] == 0 or edge[2] == 2:\n",
    "            if edge[1] not in nodes_to_append:\n",
    "                nodes_to_append.append(edge[1])\n",
    "    filtered = filter_4lang(nodes_to_append, \"en\")\n",
    "    \n",
    "    return list(set(filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expand_en_word(word):\n",
    "    data = {'word':   word}\n",
    "    data_json = json.dumps(data)\n",
    "    payload = {'json_payload': data_json}\n",
    "    headers = {'Content-type': 'application/json', 'Accept': 'text/plain'}\n",
    "    r = requests.post(\"http://hlt.bme.hu/4lang/definition\", data=data_json, headers=headers)\n",
    "    s_machines = r.json()['word']\n",
    "\n",
    "    g_sen_en = json_graph.adjacency.adjacency_graph(s_machines)\n",
    "    en_edges = utils.get_edges(g_sen_en)\n",
    "    zero_edges = defaultdict(list)\n",
    "    zero_path = []\n",
    "\n",
    "    for edge in en_edges:\n",
    "        if edge[2] == 0:\n",
    "            zero_edges[edge[0]].append(edge[1])\n",
    "            \n",
    "    edge_visited = defaultdict(bool)\n",
    "    \n",
    "    start_edges = zero_edges[word]\n",
    "    edge_visited[word] == True\n",
    "    zero_path = []\n",
    "    can_append = True\n",
    "    \n",
    "    while can_append:\n",
    "        if not start_edges:\n",
    "            can_append = False\n",
    "        zero_path += start_edges\n",
    "        help_edges = []\n",
    "        for edge in start_edges:\n",
    "            if edge_visited[edge] == False:\n",
    "                help_edges += zero_edges[edge]\n",
    "                edge_visited[edge] = True\n",
    "        edges_to_append = copy.deepcopy(help_edges)\n",
    "        start_edges = edges_to_append\n",
    "    \n",
    "    filtered = []\n",
    "    if zero_path:\n",
    "        filtered = filter_nps(zero_path, \"en\")\n",
    "        \n",
    "    return list(set(filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "for k,sentence in enumerate(sentences):\n",
    "    print(k)\n",
    "    for np in sentence['en_sen']:\n",
    "        if type(np) == tuple:\n",
    "            en_lemmas = filter_nps(np[1], \"en\")\n",
    "            for word in en_lemmas:\n",
    "                if word not in en_ancestors:\n",
    "                    filtered = expand_en_word_all(word)                   \n",
    "                    if filtered:\n",
    "                        \"\"\"\n",
    "                        for worled_to_expand in filtered:\n",
    "                            s = expand_en_word(word_to_expand)\n",
    "                            filtered_expand += s\n",
    "                        filtered += filtered_expand\n",
    "                        \"\"\"\n",
    "                        en_ancestors[word] += filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expand_hu_word_all(word):\n",
    "    word_proszeky = word\n",
    "    for k in range(len(new_char)):\n",
    "        word_proszeky = word_proszeky.replace(new_char[k], old_char[k])\n",
    "    data = {'word':   word_proszeky}\n",
    "    data_json = json.dumps(data)\n",
    "    payload = {'json_payload': data_json}\n",
    "    headers = {'Content-type': 'application/json', 'Accept': 'text/plain'}\n",
    "    r = requests.post(\"http://hlt.bme.hu/4lang/defhun\", data=data_json, headers=headers)\n",
    "    s_machines = r.json()['word']\n",
    "\n",
    "    g_sen_hu = json_graph.adjacency.adjacency_graph(s_machines)\n",
    "    hu_edges = utils.get_edges(g_sen_hu)\n",
    "    \n",
    "    nodes_to_append = []\n",
    "    \n",
    "    for edge in hu_edges:\n",
    "        if edge[2] == 0 or edge[2] == 2:\n",
    "            if edge[1] not in nodes_to_append:\n",
    "                nodes_to_append.append(edge[1])\n",
    "    filtered = filter_4lang(nodes_to_append, \"hu\")\n",
    "    \n",
    "    return list(set(filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expand_hu_word(word):\n",
    "    word_proszeky = word\n",
    "    for k in range(len(new_char)):\n",
    "        word_proszeky = word_proszeky.replace(new_char[k], old_char[k])\n",
    "    data = {'word':   word_proszeky}\n",
    "    data_json = json.dumps(data)\n",
    "    payload = {'json_payload': data_json}\n",
    "    headers = {'Content-type': 'application/json', 'Accept': 'text/plain'}\n",
    "    r = requests.post(\"http://hlt.bme.hu/4lang/defhun\", data=data_json, headers=headers)\n",
    "    s_machines = r.json()['word']\n",
    "\n",
    "    g_sen_hu = json_graph.adjacency.adjacency_graph(s_machines)\n",
    "    hu_edges = utils.get_edges(g_sen_hu)\n",
    "    zero_edges = defaultdict(list)\n",
    "    zero_path = []\n",
    "\n",
    "    for edge in hu_edges:\n",
    "        if edge[2] == 0:\n",
    "            zero_edges[edge[0]].append(edge[1])\n",
    "            \n",
    "    edge_visited = defaultdict(bool)\n",
    "    \n",
    "    start_edges = zero_edges[word_proszeky]\n",
    "    edge_visited[word_proszeky] == True\n",
    "    zero_path = []\n",
    "    can_append = True\n",
    "    \n",
    "    while can_append:\n",
    "        if not start_edges:\n",
    "            can_append = False\n",
    "        zero_path += start_edges\n",
    "        help_edges = []\n",
    "        for edge in start_edges:\n",
    "            if edge_visited[edge] == False:\n",
    "                help_edges += zero_edges[edge]\n",
    "                edge_visited[edge] = True\n",
    "        edges_to_append = copy.deepcopy(help_edges)\n",
    "        start_edges = edges_to_append\n",
    "    \n",
    "    filtered = []\n",
    "    if zero_path:\n",
    "        filtered = filter_nps(zero_path, \"hu\")\n",
    "        \n",
    "    return list(set(filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k,sentence in enumerate(sentences):\n",
    "    print(k)\n",
    "    for np in sentence['hu_sen']:\n",
    "        if type(np) == tuple:\n",
    "            hu_lemmas = filter_nps(np[1], \"hu\")\n",
    "            for word in hu_lemmas:\n",
    "                if word not in hu_ancestors:\n",
    "                    filtered = expand_hu_word_all(word)                       \n",
    "                    if filtered:\n",
    "                        \"\"\"\n",
    "                        filtered_expand = []\n",
    "                        for word_to_expand in filtered:\n",
    "                            s = expand_hu_word(word_to_expand)\n",
    "                            filtered_expand += s\n",
    "                        filtered += filtered_expand\n",
    "                        \"\"\"\n",
    "                        hu_ancestors[word] += filtered\n",
    "                    tags = morphemes[word]\n",
    "                    for tag in tags:\n",
    "                        wider_filtered = expand_hu_word_all(tag)\n",
    "                        if wider_filtered: \n",
    "                            hu_ancestors[tag] += wider_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary['additional']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hu_ancestors['látómező']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "en_ancestors['soap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fourlang_en['this']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ancestor_count = defaultdict(int)\n",
    "for ancestor in en_ancestors:\n",
    "    for a in en_ancestors[ancestor]:\n",
    "        ancestor_count[a] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_by_value = sorted(ancestor_count.items(), key=lambda kv: kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#values = [value[1] for value in sorted_by_value]\n",
    "#mean = np.array(values).mean()\n",
    "#sd = np.std(values, axis=0)\n",
    "#final_list = [x for x in sorted_by_value if (x[1] > mean - 2 * sd)]\n",
    "#final_list = [x for x in final_list if (x[1] < mean + 2 * sd)]\n",
    "en_ancestor_top = sorted_by_value[-700:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('somewhere', 4),\n",
       " ('row', 4),\n",
       " ('card', 4),\n",
       " ('therefore', 4),\n",
       " ('only', 4),\n",
       " ('vegetable', 4),\n",
       " ('effort', 4),\n",
       " ('middle', 4),\n",
       " ('example', 4),\n",
       " ('rid', 4),\n",
       " ('annoy', 4),\n",
       " ('difference', 4),\n",
       " ('net', 4),\n",
       " ('temporary', 4),\n",
       " ('ball', 4),\n",
       " ('drop', 4),\n",
       " ('improve', 4),\n",
       " ('design', 4),\n",
       " ('skilled', 4),\n",
       " ('pretty', 4),\n",
       " ('weep', 4),\n",
       " ('treatment', 4),\n",
       " ('very', 4),\n",
       " ('must', 5),\n",
       " ('copy', 5),\n",
       " ('noisy', 5),\n",
       " ('pointed', 5),\n",
       " ('mineral', 5),\n",
       " ('include', 5),\n",
       " ('defeat', 5),\n",
       " ('tie', 5),\n",
       " ('physically', 5),\n",
       " ('depend', 5),\n",
       " ('organize', 5),\n",
       " ('judge', 5),\n",
       " ('threaten', 5),\n",
       " ('unhappy', 5),\n",
       " ('consist', 5),\n",
       " ('skin', 5),\n",
       " ('general', 5),\n",
       " ('powder', 5),\n",
       " ('shiny', 5),\n",
       " ('tradition', 5),\n",
       " ('silly', 5),\n",
       " ('road', 5),\n",
       " ('wide', 5),\n",
       " ('pipe', 5),\n",
       " ('realize', 5),\n",
       " ('four', 5),\n",
       " ('solve', 5),\n",
       " ('social', 5),\n",
       " ('leaf', 5),\n",
       " ('interest', 5),\n",
       " ('interested', 5),\n",
       " ('text', 5),\n",
       " ('occasion', 5),\n",
       " ('cook', 5),\n",
       " ('suitable', 5),\n",
       " ('recognize', 5),\n",
       " ('yellowish', 5),\n",
       " ('relax', 5),\n",
       " ('purpose', 5),\n",
       " ('willing', 5),\n",
       " ('fear', 5),\n",
       " ('begin', 5),\n",
       " ('sick', 5),\n",
       " ('sensitive', 5),\n",
       " ('travel', 5),\n",
       " ('cheat', 5),\n",
       " ('leave', 5),\n",
       " ('explain', 5),\n",
       " ('loyal', 5),\n",
       " ('earth', 5),\n",
       " ('picture', 5),\n",
       " ('branch', 5),\n",
       " ('offspring', 5),\n",
       " ('rest', 5),\n",
       " ('music', 5),\n",
       " ('lot', 5),\n",
       " ('earlier', 5),\n",
       " ('notpart', 5),\n",
       " ('dishonest', 5),\n",
       " ('join', 5),\n",
       " ('ashamed', 5),\n",
       " ('enjoy', 5),\n",
       " ('list', 5),\n",
       " ('spend', 5),\n",
       " ('energy', 5),\n",
       " ('military', 5),\n",
       " ('enclose', 5),\n",
       " ('offend', 5),\n",
       " ('aggressive', 5),\n",
       " ('build', 5),\n",
       " ('badly', 5),\n",
       " ('offensive', 6),\n",
       " ('perform', 6),\n",
       " ('attack', 6),\n",
       " ('valuable', 6),\n",
       " ('hair', 6),\n",
       " ('remove', 6),\n",
       " ('mistake', 6),\n",
       " ('joint', 6),\n",
       " ('awake', 6),\n",
       " ('hole', 6),\n",
       " ('learn', 6),\n",
       " ('nervous', 6),\n",
       " ('healthy', 6),\n",
       " ('hang', 6),\n",
       " ('powerful', 6),\n",
       " ('wind', 6),\n",
       " ('new', 6),\n",
       " ('ring', 6),\n",
       " ('institution', 6),\n",
       " ('ship', 6),\n",
       " ('perfect', 6),\n",
       " ('problem', 6),\n",
       " ('weak', 6),\n",
       " ('describe', 6),\n",
       " ('harm', 6),\n",
       " ('gain', 6),\n",
       " ('cry', 6),\n",
       " ('press', 6),\n",
       " ('find', 6),\n",
       " ('worry', 6),\n",
       " ('equipment', 6),\n",
       " ('style', 6),\n",
       " ('more', 6),\n",
       " ('frame', 6),\n",
       " ('ill', 6),\n",
       " ('name', 6),\n",
       " ('christian', 6),\n",
       " ('fast', 6),\n",
       " ('trick', 6),\n",
       " ('appear', 6),\n",
       " ('severe', 6),\n",
       " ('other', 6),\n",
       " ('swing', 6),\n",
       " ('level', 6),\n",
       " ('dry', 6),\n",
       " ('longer', 6),\n",
       " ('attitude', 6),\n",
       " ('clothe', 6),\n",
       " ('film', 6),\n",
       " ('unable', 6),\n",
       " ('love', 6),\n",
       " ('disturb', 6),\n",
       " ('permanent', 6),\n",
       " ('black', 6),\n",
       " ('plane', 6),\n",
       " ('broadcast', 6),\n",
       " ('under', 6),\n",
       " ('mention', 6),\n",
       " ('prayer', 6),\n",
       " ('frequent', 6),\n",
       " ('fail', 6),\n",
       " ('contact', 6),\n",
       " ('loud', 6),\n",
       " ('breath', 7),\n",
       " ('exact', 7),\n",
       " ('movement', 7),\n",
       " ('anxious', 7),\n",
       " ('business', 7),\n",
       " ('decorate', 7),\n",
       " ('direct', 7),\n",
       " ('develop', 7),\n",
       " ('certain', 7),\n",
       " ('conscious', 7),\n",
       " ('agreement', 7),\n",
       " ('crime', 7),\n",
       " ('honest', 7),\n",
       " ('exciting', 7),\n",
       " ('friendly', 7),\n",
       " ('share', 7),\n",
       " ('price', 7),\n",
       " ('carry', 7),\n",
       " ('deal', 7),\n",
       " ('art', 7),\n",
       " ('with', 7),\n",
       " ('possible', 7),\n",
       " ('death', 7),\n",
       " ('careful', 7),\n",
       " ('wish', 7),\n",
       " ('transport', 7),\n",
       " ('woman', 7),\n",
       " ('belong', 7),\n",
       " ('remain', 7),\n",
       " ('mass', 7),\n",
       " ('company', 7),\n",
       " ('fat', 7),\n",
       " ('deceive', 7),\n",
       " ('fit', 7),\n",
       " ('principle', 7),\n",
       " ('month', 7),\n",
       " ('particular', 7),\n",
       " ('subject', 7),\n",
       " ('thread', 7),\n",
       " ('fly', 7),\n",
       " ('intend', 7),\n",
       " ('shop', 7),\n",
       " ('song', 7),\n",
       " ('kill', 8),\n",
       " ('govern', 8),\n",
       " ('catch', 8),\n",
       " ('fill', 8),\n",
       " ('cylinder', 8),\n",
       " ('sell', 8),\n",
       " ('day', 8),\n",
       " ('main', 8),\n",
       " ('well', 8),\n",
       " ('motion', 8),\n",
       " ('die', 8),\n",
       " ('curve', 8),\n",
       " ('fluid', 8),\n",
       " ('attract', 8),\n",
       " ('immediately', 8),\n",
       " ('collect', 8),\n",
       " ('pink', 8),\n",
       " ('knowledge', 8),\n",
       " ('parent', 8),\n",
       " ('tissue', 8),\n",
       " ('affect', 8),\n",
       " ('fact', 8),\n",
       " ('interesting', 8),\n",
       " ('upset', 8),\n",
       " ('argue', 8),\n",
       " ('wear', 8),\n",
       " ('dangerous', 8),\n",
       " ('circle', 8),\n",
       " ('autumn', 8),\n",
       " ('tight', 8),\n",
       " ('brown', 8),\n",
       " ('desire', 8),\n",
       " ('back', 8),\n",
       " ('fiction', 8),\n",
       " ('step', 8),\n",
       " ('split', 8),\n",
       " ('prepare', 8),\n",
       " ('frighten', 8),\n",
       " ('walk', 8),\n",
       " ('drink', 8),\n",
       " ('prevent', 8),\n",
       " ('twist', 8),\n",
       " ('story', 8),\n",
       " ('shut', 9),\n",
       " ('furniture', 9),\n",
       " ('buy', 9),\n",
       " ('stone', 9),\n",
       " ('ask', 9),\n",
       " ('quiet', 9),\n",
       " ('prove', 9),\n",
       " ('pass', 9),\n",
       " ('strange', 9),\n",
       " ('spelling', 9),\n",
       " ('gas', 9),\n",
       " ('compare', 9),\n",
       " ('wet', 9),\n",
       " ('vertebrate', 9),\n",
       " ('bird', 9),\n",
       " ('tall', 9),\n",
       " ('slow', 9),\n",
       " ('familiar', 9),\n",
       " ('rectangular', 9),\n",
       " ('base', 9),\n",
       " ('rank', 9),\n",
       " ('basic', 9),\n",
       " ('law', 9),\n",
       " ('remember', 9),\n",
       " ('loose', 9),\n",
       " ('space', 9),\n",
       " ('surprising', 9),\n",
       " ('oppose', 9),\n",
       " ('twelve', 9),\n",
       " ('impressive', 9),\n",
       " ('power', 9),\n",
       " ('consider', 9),\n",
       " ('limit', 9),\n",
       " ('burn', 10),\n",
       " ('casue', 10),\n",
       " ('religious', 10),\n",
       " ('relation', 10),\n",
       " ('brave', 10),\n",
       " ('beautiful', 10),\n",
       " ('declare', 10),\n",
       " ('politic', 10),\n",
       " ('around', 10),\n",
       " ('care', 10),\n",
       " ('book', 10),\n",
       " ('ability', 10),\n",
       " ('upright', 10),\n",
       " ('air', 10),\n",
       " ('garment', 10),\n",
       " ('since', 10),\n",
       " ('sheet', 10),\n",
       " ('polite', 10),\n",
       " ('feature', 10),\n",
       " ('border', 10),\n",
       " ('cold', 10),\n",
       " ('condition', 10),\n",
       " ('top', 10),\n",
       " ('count', 10),\n",
       " ('sad', 10),\n",
       " ('promise', 10),\n",
       " ('divide', 10),\n",
       " ('pull', 10),\n",
       " ('put', 10),\n",
       " ('choose', 10),\n",
       " ('fall', 10),\n",
       " ('fix', 10),\n",
       " ('arrange', 10),\n",
       " ('away', 10),\n",
       " ('fight', 11),\n",
       " ('far', 11),\n",
       " ('red', 11),\n",
       " ('impossible', 11),\n",
       " ('stop', 11),\n",
       " ('grey', 11),\n",
       " ('legal', 11),\n",
       " ('keep', 11),\n",
       " ('chew', 11),\n",
       " ('intense', 11),\n",
       " ('not', 11),\n",
       " ('towards', 11),\n",
       " ('special', 11),\n",
       " ('home', 11),\n",
       " ('sincere', 11),\n",
       " ('noise', 11),\n",
       " ('degree', 11),\n",
       " ('arrangement', 11),\n",
       " ('anxiety', 11),\n",
       " ('participle', 11),\n",
       " ('fruit', 11),\n",
       " ('seem', 11),\n",
       " ('nice', 11),\n",
       " ('determine', 11),\n",
       " ('smell', 12),\n",
       " ('order', 12),\n",
       " ('involve', 12),\n",
       " ('illegal', 12),\n",
       " ('cut', 12),\n",
       " ('kind', 12),\n",
       " ('doubt', 12),\n",
       " ('water', 12),\n",
       " ('green', 12),\n",
       " ('unexpected', 12),\n",
       " ('hot', 12),\n",
       " ('go', 12),\n",
       " ('be', 12),\n",
       " ('regular', 12),\n",
       " ('child', 12),\n",
       " ('active', 12),\n",
       " ('secret', 12),\n",
       " ('weapon', 12),\n",
       " ('free', 12),\n",
       " ('forbid', 12),\n",
       " ('past', 12),\n",
       " ('turn', 12),\n",
       " ('enter', 12),\n",
       " ('early', 12),\n",
       " ('read', 13),\n",
       " ('sweet', 13),\n",
       " ('dead', 13),\n",
       " ('statement', 13),\n",
       " ('tell', 13),\n",
       " ('yellow', 13),\n",
       " ('health', 13),\n",
       " ('male', 13),\n",
       " ('complex', 13),\n",
       " ('flexible', 13),\n",
       " ('offer', 13),\n",
       " ('mark', 13),\n",
       " ('bright', 13),\n",
       " ('intelligent', 13),\n",
       " ('cruel', 13),\n",
       " ('average', 14),\n",
       " ('adapt', 14),\n",
       " ('profession', 14),\n",
       " ('dimension', 14),\n",
       " ('take', 14),\n",
       " ('try', 14),\n",
       " ('behaviour', 14),\n",
       " ('teach', 14),\n",
       " ('through', 14),\n",
       " ('reproduce', 14),\n",
       " ('accept', 14),\n",
       " ('heavy', 14),\n",
       " ('repeat', 14),\n",
       " ('cloth', 14),\n",
       " ('height', 14),\n",
       " ('tool', 14),\n",
       " ('always', 14),\n",
       " ('hit', 14),\n",
       " ('stick', 14),\n",
       " ('operate', 14),\n",
       " ('attractive', 14),\n",
       " ('enough', 15),\n",
       " ('game', 15),\n",
       " ('full', 15),\n",
       " ('property', 15),\n",
       " ('influence', 15),\n",
       " ('play', 15),\n",
       " ('container', 15),\n",
       " ('ground', 15),\n",
       " ('sensible', 15),\n",
       " ('bite', 15),\n",
       " ('slight', 15),\n",
       " ('paper', 15),\n",
       " ('rule', 15),\n",
       " ('low', 15),\n",
       " ('correct', 15),\n",
       " ('receive', 15),\n",
       " ('open', 15),\n",
       " ('old', 15),\n",
       " ('satisfy', 15),\n",
       " ('image', 15),\n",
       " ('let', 15),\n",
       " ('ready', 15),\n",
       " ('responsible', 16),\n",
       " ('room', 16),\n",
       " ('organ', 16),\n",
       " ('set', 16),\n",
       " ('taste', 16),\n",
       " ('serve', 16),\n",
       " ('come', 16),\n",
       " ('unfair', 16),\n",
       " ('type', 16),\n",
       " ('front', 16),\n",
       " ('beam', 16),\n",
       " ('lead', 16),\n",
       " ('circular', 16),\n",
       " ('vehicle', 16),\n",
       " ('shape', 16),\n",
       " ('exchange', 17),\n",
       " ('event', 17),\n",
       " ('typical', 17),\n",
       " ('study', 17),\n",
       " ('stupid', 17),\n",
       " ('great', 17),\n",
       " ('dark', 17),\n",
       " ('joke', 17),\n",
       " ('hold', 17),\n",
       " ('sharp', 17),\n",
       " ('look', 18),\n",
       " ('country', 18),\n",
       " ('compete', 18),\n",
       " ('break', 18),\n",
       " ('unusual', 18),\n",
       " ('two', 18),\n",
       " ('happy', 18),\n",
       " ('serious', 18),\n",
       " ('warm', 18),\n",
       " ('sensation', 18),\n",
       " ('sure', 18),\n",
       " ('young', 19),\n",
       " ('without', 19),\n",
       " ('true', 19),\n",
       " ('allow', 19),\n",
       " ('rough', 19),\n",
       " ('artificial', 19),\n",
       " ('anger', 19),\n",
       " ('private', 19),\n",
       " ('plant', 19),\n",
       " ('process', 19),\n",
       " ('flow', 19),\n",
       " ('next', 19),\n",
       " ('normal', 19),\n",
       " ('notice', 19),\n",
       " ('machine', 19),\n",
       " ('reach', 20),\n",
       " ('moral', 20),\n",
       " ('represent', 20),\n",
       " ('visible', 20),\n",
       " ('expression', 20),\n",
       " ('produce', 20),\n",
       " ('wood', 20),\n",
       " ('deliberate', 20),\n",
       " ('stable', 21),\n",
       " ('pretend', 21),\n",
       " ('close', 21),\n",
       " ('round', 21),\n",
       " ('behave', 21),\n",
       " ('natural', 21),\n",
       " ('swallow', 21),\n",
       " ('touch', 21),\n",
       " ('admire', 21),\n",
       " ('grow', 21),\n",
       " ('become', 21),\n",
       " ('formal', 21),\n",
       " ('system', 21),\n",
       " ('destroy', 21),\n",
       " ('belief', 21),\n",
       " ('fasten', 22),\n",
       " ('white', 22),\n",
       " ('money', 22),\n",
       " ('outer', 22),\n",
       " ('unpleasant', 22),\n",
       " ('characteristic', 22),\n",
       " ('organization', 22),\n",
       " ('vertical', 22),\n",
       " ('for', 23),\n",
       " ('quick', 23),\n",
       " ('deep', 23),\n",
       " ('line', 23),\n",
       " ('fair', 23),\n",
       " ('easy', 23),\n",
       " ('treat', 23),\n",
       " ('sign', 23),\n",
       " ('female', 23),\n",
       " ('quality', 23),\n",
       " ('respect', 23),\n",
       " ('to', 24),\n",
       " ('idea', 24),\n",
       " ('demand', 24),\n",
       " ('people', 24),\n",
       " ('breathe', 24),\n",
       " ('near', 25),\n",
       " ('word', 25),\n",
       " ('much', 25),\n",
       " ('device', 25),\n",
       " ('direction', 25),\n",
       " ('enjoyable', 26),\n",
       " ('angry', 26),\n",
       " ('house', 26),\n",
       " ('short', 26),\n",
       " ('metal', 26),\n",
       " ('plan', 26),\n",
       " ('damage', 26),\n",
       " ('sex', 26),\n",
       " ('pay', 26),\n",
       " ('can', 26),\n",
       " ('liquid', 26),\n",
       " ('period', 26),\n",
       " ('understand', 26),\n",
       " ('help', 26),\n",
       " ('conduct', 26),\n",
       " ('information', 26),\n",
       " ('out', 26),\n",
       " ('agree', 27),\n",
       " ('empty', 27),\n",
       " ('succeed', 27),\n",
       " ('many', 28),\n",
       " ('hear', 28),\n",
       " ('wrong', 28),\n",
       " ('form', 28),\n",
       " ('narrow', 28),\n",
       " ('decide', 28),\n",
       " ('express', 28),\n",
       " ('warn', 28),\n",
       " ('series', 29),\n",
       " ('contain', 29),\n",
       " ('temperature', 29),\n",
       " ('support', 29),\n",
       " ('complete', 29),\n",
       " ('high', 30),\n",
       " ('from', 30),\n",
       " ('smooth', 30),\n",
       " ('straight', 30),\n",
       " ('measure', 31),\n",
       " ('continue', 31),\n",
       " ('likely', 31),\n",
       " ('on', 32),\n",
       " ('emotion', 33),\n",
       " ('often', 33),\n",
       " ('way', 33),\n",
       " ('mean', 33),\n",
       " ('control', 33),\n",
       " ('whole', 34),\n",
       " ('pleasant', 34),\n",
       " ('able', 34),\n",
       " ('confident', 35),\n",
       " ('mammal', 35),\n",
       " ('clear', 35),\n",
       " ('right', 35),\n",
       " ('about', 36),\n",
       " ('public', 36),\n",
       " ('usual', 36),\n",
       " ('talk', 37),\n",
       " ('shine', 37),\n",
       " ('time', 37),\n",
       " ('food', 38),\n",
       " ('between', 38),\n",
       " ('firm', 38),\n",
       " ('perceive', 40),\n",
       " ('opinion', 40),\n",
       " ('real', 40),\n",
       " ('get', 40),\n",
       " ('activity', 40),\n",
       " ('useful', 41),\n",
       " ('flat', 42),\n",
       " ('position', 43),\n",
       " ('show', 43),\n",
       " ('surround', 43),\n",
       " ('point', 43),\n",
       " ('physical', 43),\n",
       " ('violent', 43),\n",
       " ('thick', 43),\n",
       " ('building', 43),\n",
       " ('life', 44),\n",
       " ('political', 44),\n",
       " ('bend', 44),\n",
       " ('situation', 45),\n",
       " ('area', 45),\n",
       " ('thin', 45),\n",
       " ('write', 46),\n",
       " ('common', 46),\n",
       " ('artefact', 47),\n",
       " ('notin', 47),\n",
       " ('land', 48),\n",
       " ('like', 49),\n",
       " ('extreme', 49),\n",
       " ('believe', 50),\n",
       " ('eat', 50),\n",
       " ('official', 51),\n",
       " ('relate', 51),\n",
       " ('need', 52),\n",
       " ('sudden', 52),\n",
       " ('substance', 53),\n",
       " ('rigid', 53),\n",
       " ('notat', 54),\n",
       " ('something', 54),\n",
       " ('soft', 55),\n",
       " ('etc', 56),\n",
       " ('human', 57),\n",
       " ('conform', 57),\n",
       " ('light', 58),\n",
       " ('protect', 58),\n",
       " ('difficult', 58),\n",
       " ('solid', 58),\n",
       " ('amount', 59),\n",
       " ('act', 60),\n",
       " ('surface', 62),\n",
       " ('state', 63),\n",
       " ('piece', 64),\n",
       " ('structure', 64),\n",
       " ('strong', 64),\n",
       " ('hard', 64),\n",
       " ('cover', 66),\n",
       " ('exist', 67),\n",
       " ('say', 67),\n",
       " ('live', 70),\n",
       " ('before', 70),\n",
       " ('nothas', 71),\n",
       " ('speak', 71),\n",
       " ('action', 74),\n",
       " ('see', 74),\n",
       " ('communicate', 75),\n",
       " ('man', 76),\n",
       " ('animal', 77),\n",
       " ('unit', 80),\n",
       " ('colour', 80),\n",
       " ('large', 84),\n",
       " ('different', 84),\n",
       " ('happen', 85),\n",
       " ('give', 86),\n",
       " ('notcause', 86),\n",
       " ('similar', 87),\n",
       " ('separate', 88),\n",
       " ('sound', 90),\n",
       " ('work', 93),\n",
       " ('material', 94),\n",
       " ('bad', 98),\n",
       " ('quantity', 102),\n",
       " ('together', 104),\n",
       " ('horizontal', 105),\n",
       " ('one', 105),\n",
       " ('member', 105),\n",
       " ('think', 112),\n",
       " ('do', 114),\n",
       " ('follow', 115),\n",
       " ('specific', 116),\n",
       " ('long', 118),\n",
       " ('thing', 121),\n",
       " ('know', 128),\n",
       " ('place', 128),\n",
       " ('person', 129),\n",
       " ('after', 133),\n",
       " ('change', 134),\n",
       " ('number', 135),\n",
       " ('big', 149),\n",
       " ('especially', 151),\n",
       " ('expect', 155),\n",
       " ('want', 175),\n",
       " ('size', 180),\n",
       " ('feel', 182),\n",
       " ('important', 184),\n",
       " ('group', 193),\n",
       " ('small', 198),\n",
       " ('good', 217),\n",
       " ('move', 221),\n",
       " ('object', 239),\n",
       " ('use', 293),\n",
       " ('make', 303),\n",
       " ('at', 336),\n",
       " ('in', 396),\n",
       " ('part', 467),\n",
       " ('er', 532),\n",
       " ('instrument', 626),\n",
       " ('', 686),\n",
       " ('have', 737),\n",
       " ('lack', 868),\n",
       " ('because', 1211)]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_ancestor_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "en_ancestor_top = [word[0] for word in en_ancestor_top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ancestor_count = defaultdict(int)\n",
    "for ancestor in hu_ancestors:\n",
    "    for a in hu_ancestors[ancestor]:\n",
    "        ancestor_count[a] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_by_value = sorted(ancestor_count.items(), key=lambda kv: kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#values = [value[1] for value in sorted_by_value]\n",
    "#mean = np.array(values).mean()\n",
    "#sd = np.std(values, axis=0)\n",
    "#final_list = [x for x in sorted_by_value if (x[1] > mean - 2 * sd)]\n",
    "#final_list = [x for x in final_list if (x[1] < mean + 2 * sd)]\n",
    "hu_ancestor_top = sorted_by_value[-500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hu_ancestor_top = [word[0] for word in hu_ancestor_top]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 as es kettes éleket behozva, top 500 őst levagva: behuzott élek: 2815, precision: 72,78, recall: 73,91, f_score 73,34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "az egesz adaton: behuzott élek: 18036, precision: 76,99, recall: 73,67, f_score: 75,3\n",
    "\n",
    "Moprh nélkül, kétféle filter 4langra és sima NP-re: élek: 18691, precision 0.7583863891712589 recall 0.7520691850594228,\n",
    "f1_score: 0.7552145768400864\n",
    "\n",
    "Morphal együtt: \n",
    "0.7476161493203489\n",
    "18848\n",
    "19716\n",
    "0.7820458404074703\n",
    "0.7644435224561766"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('en_ancestors_t.json', 'w+') as fp:\n",
    "    json.dump(en_ancestors, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "with open('hu_ancestors.json') as f:\n",
    "    hu_ancestors = json.load(f)\n",
    "with open('en_ancestors.json') as f:\n",
    "    en_ancestors = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
